---

layout: post
title: Spark 泛谈
category: 技术
tags: Compute
keywords: Spark

---

## 前言 

在 2012 年，UC 伯克利 AMP 实验室（Algorithms、Machine 和 People 的缩写）开发的 Spark 开始崭露头角。当时 AMP 实验室的马铁博士发现使用 MapReduce 进行机器学习计算的时候性能非常差，因为机器学习算法通常需要进行很多次的迭代计算，而 MapReduce 每执行一次 Map 和 Reduce 计算都需要重新启动一次作业，带来大量的无谓消耗。还有一点就是 MapReduce 主要使用磁盘作为存储介质，而 2012 年的时候，内存已经突破容量和成本限制，成为数据运行过程中主要的存储介质。

## 从操作上直观感受 spark

### 单机模式

1. 官网下载spark-2.3.0-bin-hadoop2.7.tgz包，解压
2. 启动master，`spark-2.3.0-bin-hadoop2.7/sbin/start-master.sh`
3. 可以在 `http://localhost:8080` 下查看 webui
4. 启动一个slave，`spark-2.3.0-bin-hadoop2.7/sbin/start-slave.sh spark://localhost:7077`

![](/public/upload/data/spark_ui.png)

从启动过程及web ui 上可以看到

1. 跟mesos 很像，mesos 也是start-master,start-slave，mesos master 在5050 端口提供ui。启动一个slave后，worker 列表新增了一行元素，包括了地址、状态、cpu核数及内存。跟mesos 的agent 列表如出一辙
2. 与hadoop2.x 类似，spark 也分为资源管理 + 任务调度

	||master|slave|
	|---|---|---|
	|yarn|ResourceManager|NodeManager|
	|mesos|master|salve|
	|spark|master|slave|


### 交互式查询

传统的 通过提交代码与 spark 或 mapreduce 交互

一个mapreduce 任务执行的流程

1. 编写代码
2. 打成jar 包
3. hadoop master 机器上 `hadoop jar wordcount.jar input_arg output_arg`

对应到 spark 则是

1. 编写代码
2. `bin/spark-submit --class xx.xx.wordcount target_jar input_arg out_arg`


《Spark快速大数据分析》讲到spark shell时提到：使用其它shell工具，你只能用单机的硬盘和内存来操作数据，而Spark Shell可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由spark自动控制完成（spark 速度快，速度快就意味着我们可以进行 交互式的数据操作）。

	scala> val input = sc.textFile("/tmp/inputFile")
	spark info...
	scala> val words = input.flatMap(line => line.split(" "))
	spark info...
	scala> val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
	spark info...
	scala> counts.saveAsTextFile("/tmp/output")
	spark info...

此处，运行完毕后，`/tmp/output` 是一个目录，其结构 跟 hdfs 是一样一样的，这跟选用saveAsTextFile方法有关系。

## rdd——spark和MR的最大不同

大数据计算就是在大规模的数据集上进行一系列的数据计算处理。MapReduce 针对输入数据，将计算过程分为两个阶段，一个 Map 阶段，一个 Reduce 阶段，可以理解成是**面向过程的大数据计算**。我们在用 MapReduce 编程的时候，思考的是，如何将计算逻辑用 Map 和 Reduce 两个阶段实现，**map 和 reduce 函数的输入和输出是什么**，这也是我们在学习 MapReduce 编程的时候一再强调的。而 Spark 则直接针对数据进行编程，将大规模数据集合抽象成一个 RDD 对象，然后在这个 RDD 上进行各种计算处理，得到一个新的 RDD，继续计算处理，直到得到最后的结果数据。所以 Spark 可以理解成是**面向对象的大数据计算**。我们在进行 Spark 编程的时候，思考的是一个 RDD 对象需要经过什么样的操作，转换成另一个 RDD 对象，思考的重心和落脚点都在 RDD 上。PS： spark无论处理什么数据先整成一个rdd 再说。

```scala
val rdd1 = textFile.flatMap(line => line.split(" "))
val rdd2 = rdd1.map(word => (word, 1))
val rdd3 = rdd2.reduceByKey(_ + _)
```

**RDD非常类似于 tensorflow 中的dataset 与 tensor 的结合体**，从内部或外部数据源获取RDD，算子也都Apply 在RDD 之上。分布式计算的精髓，在于如何把抽象的计算流图，转化为实实在在的分布式计算任务，然后以并行计算的方式交付执行。具体的说，RDD 的计算以数据分区为粒度，依照算子的逻辑，Executors 以相互独立的方式，完成不同数据分区的计算与转换。PS： 跟 MapReduce 一样，Spark 也是对大数据进行分片计算，Spark 分布式计算的数据分片、任务调度都是以 RDD 为单位展开的，每个 RDD 分片都会分配到一个执行进程去处理。

![](/public/upload/data/spark_vs_mapreduce.png)

而RDD无需文件来存储中间结果，所以hadoop操作和RDD有所不同，RDD的形式可以更丰富。

1. rdd 支持两种操作： Transformations 操作和 Actions操作。RDD 上的转换操作又分成两种
	1. 转换操作产生的 RDD 不会出现新的分片，比如 map、filter 等，也就是说一个 RDD 数据分片，经过 map 或者 filter 转换操作后，结果还在当前分片。就像你用 map 函数对每个数据加 1，得到的还是这样一组数据，只是值不同。
	2. 转换操作产生的 RDD 则会产生新的分片，比如reduceByKey，来自不同分片的相同 Key 必须聚合在一起进行操作，这样就会产生新的 RDD 分片。
2. 惰性求值，在 Actions操作开始之前，spark 不会开始 Transformations操作。
3. 转化操作 会返回一个新的rdd，老的rdd 数据不会被改变
4. rdd 根据转换操作 形成lineage graph，每当调用一个行动操作，lineage graph 都会从头开始计算


在这样的编程模型下，Spark 在运行时的计算被划分为两个环节。PS： 与flink/tf的dag 也很像，采用Lazy evaluation（对应Eager evaluation）
1. 基于不同数据形态之间的转换，构建计算流图（DAG，Directed Acyclic Graph）；
2. 通过 Actions 类算子，以回溯的方式去触发执行这个计算流图。

## 整体架构

Spark 应用程序代码中的 RDD 和 Spark 执行过程中生成的物理 RDD 不是一一对应的。PS： 这点与tf 更像，Spark Core引人RDD的概念更多的是把数据处理步骤组成的有向无环图（DAG）抽象成类似函数式编程中的集合的概念，而把分布式数据处理的过程隐藏在这个抽象后面，比如划分stage，划分task，shuffle，调度这些task，保证data locality等等。

Spark 程序由 Manager Node（管理节点）进行调度组织，由 Worker Node（工作节点）进行具体的计算任务执行，最终将结果返回给 Drive Program（驱动程序）。在物理的 Worker Node 上，数据还会分为不同的 partition（数据分片），可以说 partition 是 Spark 的基础数据单元。

![](/public/upload/compute/spark_overview.png)

我们用一个任务来解释一下 Spark 的工作过程：我们需要先从本地硬盘读取文件 textFile，再从分布式文件系统 HDFS 读取文件 hadoopFile，然后分别对它们进行处理，再把两个文件按照 ID 都 join 起来得到最终的结果。

![](/public/upload/compute/spark_demo.png)

DAGScheduler 核心职责，根据程序代码生成 DAG，把计算图 DAG 拆分为执行阶段 Stages，Stages 指的是不同的运行阶段，同时还要负责把 Stages 转化为任务集合 TaskSets。
1. 用一句话来概括从 DAG 到 Stages 的拆分过程：**以 Shuffle 为边界**，从后向前以递归的方式，把逻辑上的计算图 DAG，转化成一个又一个 Stages。PS： 计算阶段划分的依据是 shuffle，不是转换函数的类型
2. 从后向前，以递归的方式，依次提请执行所有的 Stages。在 Word Count 的例子中，DAGScheduler 最先提请执行的是 Stage1。在提交的时候，DAGScheduler 发现 Stage1 依赖的父 Stage，也就是 Stage0，还没有执行过，那么这个时候它会把 Stage1 的提交动作压栈，转而去提请执行 Stage0。当 Stage0 执行完毕的时候，DAGScheduler 通过出栈的动作，再次提请执行 Stage 1。
3. 对于提请执行的每一个 Stage，DAGScheduler 根据 Stage 内 RDD 的 partitions 属性创建分布式任务集合 TaskSet。TaskSet 包含一个又一个分布式任务 Task，RDD 有多少数据分区，TaskSet 就包含多少个 Task。换句话说，Task 与 RDD 的分区，是一一对应的。
4. Task 代表的是分布式任务，包含以下属性。 
	1. stageId,  task 所在stage
	2. stageAttemptId, 失败重试编号
	3. taskBinary, 任务代码。broadcasted version of the serialized RDD and the function to apply on each partition of the given RDD, once deserialized, the type should be `(RDD[T],(TaskContext,Iterator[T])=>U)`。PS： 对象序列化之后发给executor，executor 发序列化后执行。 
	4. partition, task 对应的RDD 分区
	5. locs, 以字符串的形式记录了该任务倾向的计算节点或是 Executor ID
	taskBinary、partition 和 locs 这三个属性，一起描述了这样一件事情：Task 应该在哪里（locs）为谁（partition）执行什么任务（taskBinary）。

SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构 `<标记 Executor 的字符串,ExecutorData>`，ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等。SchedulerBackend 与集群内所有 Executors 中的 ExecutorBackend 保持周期性通信，双方通过 LaunchedExecutor、RemoveExecutor、StatusUpdate 等消息来互通有无、变更可用计算资源。

对于给定的 WorkerOffer，TaskScheduler 是按照任务的本地倾向性，来遴选出 TaskSet 中适合调度的 Tasks。TaskScheduler 就把这些 Tasks 通过 LaunchTask 消息发送给 SchedulerBackend。 SchedulerBackend 拿到这些活儿之后，同样使用 LaunchTask 消息，把活儿进一步下发给ExecutorBackend。PS： SchedulerBackend 知道哪些节点有多少资源（对外提供WorkerOffer），DAGScheduler 产出TaskSet，TaskScheduler 做适配

ExecutorBackend 拿到“活儿”之后，随即把活儿派发给Executor，Executor 先检查自己是否有 Driver 的执行代码，如果没有，**从 Driver 下载执行代码，通过 Java 反射加载后开始执行**。每个线程负责处理一个 Task，每当 Task 处理完毕，这些线程便会通过 ExecutorBackend，向 Driver 端的 SchedulerBackend 发送 StatusUpdate 事件，告知 Task 执行状态。

![](/public/upload/compute/spark_run.png)

### 与MapReduce 对比

和 MapReduce 一个应用一次只运行一个 map 和一个 reduce 不同，Spark 可以根据应用的复杂程度，分割成更多的计算阶段（stage），这些计算阶段组成一个有向无环图 DAG，Spark 任务调度器可以根据 DAG 的依赖关系执行计算阶段。你可以看到 Spark 作业调度执行的核心是 DAG，有了 DAG，整个应用就被切分成哪些阶段，每个阶段的依赖关系也就清楚了。之后再根据每个阶段要处理的数据量生成相应的任务集合（TaskSet），每个任务都分配一个任务进程去处理，Spark 就实现了大数据的分布式计算。

其实从本质上看，Spark 可以算作是一种 MapReduce 计算模型的不同实现。Hadoop MapReduce 简单粗暴地根据 shuffle 将大数据计算分成 Map 和 Reduce 两个阶段，然后就算完事了。而 Spark 更细腻一点，将前一个的 Reduce 和后一个的 Map 连接起来，当作一个阶段持续计算（如果一个应用的计算阶段变得很多的话，MapReduce需要启动多个应用进行计算），形成一个更加优雅、高效的计算模型，虽然其本质依然是 Map 和 Reduce。但是这种多个计算阶段依赖执行的方案可以有效减少对 HDFS 的访问，减少作业的调度执行次数，因此执行速度也更快。

并且和 Hadoop MapReduce 主要使用磁盘存储 shuffle 过程中的数据不同，Spark 优先使用内存进行数据存储，包括 RDD 数据。除非是内存不够用了，否则是尽可能使用内存， 这也是 Spark 性能比 Hadoop 高的另一个原因。

总结来说，Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效。

## 代码的运行

### 上层应用

spark 可以使用自己的 master/slave，也可以使用mesos 或 hadoop yarn

spark/mapreduce 作为 yarn/mesos 的上层，有自己的spark "Scheduler"和 spark "Executor"，对于一段spark 代码
	
	object WordCount {
	    def main(args: Array[String]): Unit = {
	        val conf = new SparkConf().setAppName("wordCount");
	        val sc = new SparkContext(conf)
	        val input = sc.textFile("/Users/nali/tmp/hello")
	        val words = input.flatMap(line => line.split(" "))
	        val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
	        counts.saveAsTextFile("/Users/nali/tmp/output")
	    }
	}

它实际 是一个 独立运行的进程么？它和spark master如何交互呢？

[Spark 学习: spark 原理简述与 shuffle 过程介绍](https://blog.csdn.net/databatman/article/details/53023818) 

要点如下：

1. wordcount 会对应一个driver 进程，executor 由 spark 框架提供。driver 和 Executor 就是 wordcount 应用的 "scheduler" 和 "executor"
2. Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批Task，然后将这些Task分配到各个Executor进程中执行。
3. Task是最小的计算单元（以线程方式执行）。前文提到资源管理器 就是帮你启动Scheduler、Executor，并提供通信服务，Executor 就是启动和监控task。于是，**上层是抽象的rdd接口，下层是一个个task**， 中间这种抽象层次的弥合 便通过driver （也就是Scheduler ）实现。 


### shuffle

DAGScheduler 以 Shuffle 为边界，把计算图 DAG 切割为多个执行阶段 Stages。显然，Shuffle 是这个环节的关键。Shuffle 的本意是扑克的“洗牌”，在分布式计算场景中，它被引申为集群范围内跨节点、跨进程的数据分发，会引入大量的磁盘 I/O 与网络 I/O。以 Shuffle 为边界，reduceByKey 的计算被切割为两个执行阶段。约定俗成地，我们把 Shuffle 之前的 Stage 叫作 Map 阶段，而把 Shuffle 之后的 Stage 称作 Reduce 阶段。
1. 在 Map 阶段，每个 Executors 先把自己负责的数据分区做初步聚合（又叫 Map 端聚合、局部聚合）；
2. 在 Shuffle 环节，不同的单词被分发到不同节点的 Executors 中；
3. 在 Reduce 阶段，Executors 以单词为 Key 做第二次聚合（又叫全局聚合），从而完成统计计数的任务。

与其说 Shuffle 是跨节点、跨进程的数据分发，不如说 Shuffle 是 Map 阶段与 Reduce 阶段之间的数据交换。如何实现数据交换的呢？Map 阶段与 Reduce 阶段，通过生产与消费 Shuffle 中间文件的方式，来完成集群范围内的数据交换。换句话说，Map 阶段生产 Shuffle 中间文件，Reduce 阶段消费 Shuffle 中间文件，二者以中间文件为媒介，完成数据交换。
1. Shuffle 文件的生成，是以 Map Task 为粒度的，Map 阶段有多少个 Map Task，就会生成多少份 Shuffle 中间文件。
2. 在生成中间文件的过程中，Spark 会借助一种类似于 Map 的数据结构，来计算、缓存并排序数据分区中的数据记录。这种 Map 结构的 Key 是（Reduce Task Partition ID，Record Key），而 Value 是原数据记录中的数据值。当 Map 结构被灌满之后，Spark 根据主键对 Map 中的数据记录做排序，然后把所有内容溢出到磁盘中的临时文件，如此往复，直到数据分区中所有的数据记录都被处理完毕。到此为止，磁盘上存有若干个溢出的临时文件，而内存的 Map 结构中留有部分数据，Spark 使用归并排序算法对所有临时文件和 Map 结构剩余数据做合并，分别生成 data 文件、和与之对应的 index 文件（用来标记目标分区所属数据记录的起始索引）。Shuffle 阶段生成中间文件的过程，又叫 Shuffle Write。
3. 不同的 Reduce Task 正是根据 index 文件中的起始索引来确定哪些数据内容是“属于自己的”。

### rdd 和block

Spark 存储系统负责维护所有暂存在内存与磁盘中的数据，这些数据包括 Shuffle 中间文件、RDD Cache 以及广播变量。[Spark存储系统原理与源码解析](https://mp.weixin.qq.com/s/1OUAvXgKuMlyAxcqcJc_3Q)

![](/public/upload/compute/spark_block.png)

[Spark自己的分布式存储系统BlockManager全解析](https://mp.weixin.qq.com/s/0XUeE6JWMRBSXSPJQFCmcw)BlockManager 是一个嵌入在 spark 中的 key-value型分布式存储系统，BlockManager 在一个 spark 应用中作为一个本地缓存运行在所有的节点上，BlockManager 对本地和远程提供一致的 get 和set 数据块接口，BlockManager 本身使用不同的存储方式来存储这些数据， 包括 memory, disk, off-heap。BlockManager内部会创建出MemoryStore和DiskStore对象用以存取block，如果内存中拥有足够的内存， 就 使用 MemoryStore存储，  如果 不够， 就 spill 到 磁盘中， 通过 DiskStore进行存储。

```scala
def putBlockData(blockId: BlockId,data: ManagedBuffer,level: StorageLevel,classTag: ClassTag[_]): Boolean = {...}
def get[T: ClassTag](blockId: BlockId): Option[BlockResult] = {...}
```

1. 在RDD层面上，RDD是由不同的partition组成的，所进行的transformation和action是在partition上面进行的；
2. 在storage模块内部，RDD又被视为由不同的block组成，对于RDD的存取是以block为单位进行的

本质上partition和block是等价的，只是看待的角度不同。在Spark storage模块中中存取数据的最小单位是block，所有的操作都是以block为单位进行的。RDD 的运算是基于 partition， 每个 task 代表一个 分区上一个 stage 内的运算闭包， task 被分别调度到 多个 executor上去运行， 那么是在哪里变成了 Block 呢，  我们以 spark 2.11 源码为准， 看看这个转变过程，一个 RDD 调度到 executor 上会运行调用 getOrCompute方法。
```scala
SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => {
      readCachedBlock = false
      computeOrReadCheckpoint(partition, context)
    })
```
如果当前RDD的storage level不是NONE的话，表示该RDD在BlockManager中有存储，那么调用CacheManager中的getOrCompute()函数计算RDD，在这个函数中partition和block发生了关系：首先根据RDD id和partition index构造出block id (rdd_xx_xx)，接着从BlockManager中取出相应的block。

