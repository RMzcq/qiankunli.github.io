---

layout: post
title: Spark 泛谈
category: 技术
tags: Compute
keywords: Spark

---

## 前言 

## 从操作上直观感受 spark

### 单机模式

1. 官网下载spark-2.3.0-bin-hadoop2.7.tgz包，解压
2. 启动master，`spark-2.3.0-bin-hadoop2.7/sbin/start-master.sh`
3. 可以在 `http://localhost:8080` 下查看 webui
4. 启动一个slave，`spark-2.3.0-bin-hadoop2.7/sbin/start-slave.sh spark://localhost:7077`

![](/public/upload/data/spark_ui.png)

从启动过程及web ui 上可以看到

1. 跟mesos 很像，mesos 也是start-master,start-slave，mesos master 在5050 端口提供ui。启动一个slave后，worker 列表新增了一行元素，包括了地址、状态、cpu核数及内存。跟mesos 的agent 列表如出一辙
2. 与hadoop2.x 类似，spark 也分为资源管理 + 任务调度

	||master|slave|
	|---|---|---|
	|yarn|ResourceManager|NodeManager|
	|mesos|master|salve|
	|spark|master|slave|


### 交互式查询

传统的 通过提交代码与 spark 或 mapreduce 交互

一个mapreduce 任务执行的流程

1. 编写代码
2. 打成jar 包
3. hadoop master 机器上 `hadoop jar wordcount.jar input_arg output_arg`

对应到 spark 则是

1. 编写代码
2. `bin/spark-submit --class xx.xx.wordcount target_jar input_arg out_arg`


《Spark快速大数据分析》讲到spark shell时提到：使用其它shell工具，你只能用单机的硬盘和内存来操作数据，而Spark Shell可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由spark自动控制完成（spark 速度快，速度快就意味着我们可以进行 交互式的数据操作）。

	scala> val input = sc.textFile("/tmp/inputFile")
	spark info...
	scala> val words = input.flatMap(line => line.split(" "))
	spark info...
	scala> val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
	spark info...
	scala> counts.saveAsTextFile("/tmp/output")
	spark info...

此处，运行完毕后，`/tmp/output` 是一个目录，其结构 跟 hdfs 是一样一样的，这跟选用saveAsTextFile方法有关系。



## rdd——spark和MR的最大不同

**非常类似于 tensorflow 中的dataset 与 tensor 的结合体**，从内部或外部数据源获取RDD，算子也都Apply 在RDD 之上。分布式计算的精髓，在于如何把抽象的计算流图，转化为实实在在的分布式计算任务，然后以并行计算的方式交付执行。具体的说，RDD 的计算以数据分区为粒度，依照算子的逻辑，Executors 以相互独立的方式，完成不同数据分区的计算与转换。

![](/public/upload/data/spark_vs_mapreduce.png)

而RDD无需文件来存储中间结果，所以hadoop操作和RDD有所不同，RDD的形式可以更丰富。

1. rdd 支持两种操作： Transformations 操作和 Actions操作
2. 惰性求值，在 Actions操作开始之前，spark 不会开始 Transformations操作。
3. 转化操作 会返回一个新的rdd，老的rdd 数据不会被改变
4. rdd 根据转换操作 形成lineage graph，每当调用一个行动操作，lineage graph 都会从头开始计算

||存储|提供操作|
|---|----|----|
|mapreduce|hdfs|map/reduce|
|sparck|RDD|transform,actiion|

在这样的编程模型下，Spark 在运行时的计算被划分为两个环节。PS： 与flink/tf的dag 也很像，采用Lazy evaluation（对应Eager evaluation）
1. 基于不同数据形态之间的转换，构建计算流图（DAG，Directed Acyclic Graph）；
2. 通过 Actions 类算子，以回溯的方式去触发执行这个计算流图。

## 整体架构

Spark 程序由 Manager Node（管理节点）进行调度组织，由 Worker Node（工作节点）进行具体的计算任务执行，最终将结果返回给 Drive Program（驱动程序）。在物理的 Worker Node 上，数据还会分为不同的 partition（数据分片），可以说 partition 是 Spark 的基础数据单元。

![](/public/upload/compute/spark_overview.png)

我们用一个任务来解释一下 Spark 的工作过程：我们需要先从本地硬盘读取文件 textFile，再从分布式文件系统 HDFS 读取文件 hadoopFile，然后分别对它们进行处理，再把两个文件按照 ID 都 join 起来得到最终的结果。

![](/public/upload/compute/spark_demo.png)

DAGScheduler 核心职责，是把计算图 DAG 拆分为执行阶段 Stages，Stages 指的是不同的运行阶段，同时还要负责把 Stages 转化为任务集合 TaskSets。
1. 用一句话来概括从 DAG 到 Stages 的拆分过程：以 Shuffle 为边界，从后向前以递归的方式，把逻辑上的计算图 DAG，转化成一个又一个 Stages。
2. 从后向前，以递归的方式，依次提请执行所有的 Stages。在 Word Count 的例子中，DAGScheduler 最先提请执行的是 Stage1。在提交的时候，DAGScheduler 发现 Stage1 依赖的父 Stage，也就是 Stage0，还没有执行过，那么这个时候它会把 Stage1 的提交动作压栈，转而去提请执行 Stage0。当 Stage0 执行完毕的时候，DAGScheduler 通过出栈的动作，再次提请执行 Stage 1。
3. 对于提请执行的每一个 Stage，DAGScheduler 根据 Stage 内 RDD 的 partitions 属性创建分布式任务集合 TaskSet。TaskSet 包含一个又一个分布式任务 Task，RDD 有多少数据分区，TaskSet 就包含多少个 Task。换句话说，Task 与 RDD 的分区，是一一对应的。
4. Task 代表的是分布式任务，包含以下属性。 
	1. stageId,  task 所在stage
	2. stageAttemptId, 失败重试编号
	3. taskBinary, 任务代码。broadcasted version of the serialized RDD and the function to apply on each partition of the given RDD, once deserialized, the type should be `(RDD[T],(TaskContext,Iterator[T])=>U)`。PS： 对象序列化之后发给executor，executor 发序列化后执行。 
	4. partition, task 对应的RDD 分区
	5. locs, 以字符串的形式记录了该任务倾向的计算节点或是 Executor ID
	taskBinary、partition 和 locs 这三个属性，一起描述了这样一件事情：Task 应该在哪里（locs）为谁（partition）执行什么任务（taskBinary）。

SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构 `<标记 Executor 的字符串,ExecutorData>`，ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等。SchedulerBackend 与集群内所有 Executors 中的 ExecutorBackend 保持周期性通信，双方通过 LaunchedExecutor、RemoveExecutor、StatusUpdate 等消息来互通有无、变更可用计算资源。

对于给定的 WorkerOffer，TaskScheduler 是按照任务的本地倾向性，来遴选出 TaskSet 中适合调度的 Tasks。TaskScheduler 就把这些 Tasks 通过 LaunchTask 消息发送给 SchedulerBackend。 SchedulerBackend 拿到这些活儿之后，同样使用 LaunchTask 消息，把活儿进一步下发给ExecutorBackend。PS： SchedulerBackend 知道哪些节点有多少资源（对外提供WorkerOffer），DAGScheduler 产出TaskSet，TaskScheduler 做适配

ExecutorBackend 拿到“活儿”之后，随即把活儿派发给Executor，每个线程负责处理一个 Task。每当 Task 处理完毕，这些线程便会通过 ExecutorBackend，向 Driver 端的 SchedulerBackend 发送 StatusUpdate 事件，告知 Task 执行状态。

![](/public/upload/compute/spark_run.png)

## 代码的运行

### 上层应用

spark 可以使用自己的 master/slave，也可以使用mesos 或 hadoop yarn

spark/mapreduce 作为 yarn/mesos 的上层，有自己的spark "Scheduler"和 spark "Executor"，对于一段spark 代码
	
	object WordCount {
	    def main(args: Array[String]): Unit = {
	        val conf = new SparkConf().setAppName("wordCount");
	        val sc = new SparkContext(conf)
	        val input = sc.textFile("/Users/nali/tmp/hello")
	        val words = input.flatMap(line => line.split(" "))
	        val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
	        counts.saveAsTextFile("/Users/nali/tmp/output")
	    }
	}

它实际 是一个 独立运行的进程么？它和spark master如何交互呢？

[Spark 学习: spark 原理简述与 shuffle 过程介绍](https://blog.csdn.net/databatman/article/details/53023818) 

要点如下：

1. wordcount 会对应一个driver 进程，executor 由 spark 框架提供。driver 和 Executor 就是 wordcount 应用的 "scheduler" 和 "executor"
2. Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批Task，然后将这些Task分配到各个Executor进程中执行。
3. Task是最小的计算单元（以线程方式执行）。前文提到资源管理器 就是帮你启动Scheduler、Executor，并提供通信服务，Executor 就是启动和监控task。于是，**上层是抽象的rdd接口，下层是一个个task**， 中间这种抽象层次的弥合 便通过driver （也就是Scheduler ）实现。 


### shuffle

DAGScheduler 以 Shuffle 为边界，把计算图 DAG 切割为多个执行阶段 Stages。显然，Shuffle 是这个环节的关键。Shuffle 的本意是扑克的“洗牌”，在分布式计算场景中，它被引申为集群范围内跨节点、跨进程的数据分发，会引入大量的磁盘 I/O 与网络 I/O。以 Shuffle 为边界，reduceByKey 的计算被切割为两个执行阶段。约定俗成地，我们把 Shuffle 之前的 Stage 叫作 Map 阶段，而把 Shuffle 之后的 Stage 称作 Reduce 阶段。
1. 在 Map 阶段，每个 Executors 先把自己负责的数据分区做初步聚合（又叫 Map 端聚合、局部聚合）；
2. 在 Shuffle 环节，不同的单词被分发到不同节点的 Executors 中；
3. 在 Reduce 阶段，Executors 以单词为 Key 做第二次聚合（又叫全局聚合），从而完成统计计数的任务。

与其说 Shuffle 是跨节点、跨进程的数据分发，不如说 Shuffle 是 Map 阶段与 Reduce 阶段之间的数据交换。如何实现数据交换的呢？Map 阶段与 Reduce 阶段，通过生产与消费 Shuffle 中间文件的方式，来完成集群范围内的数据交换。换句话说，Map 阶段生产 Shuffle 中间文件，Reduce 阶段消费 Shuffle 中间文件，二者以中间文件为媒介，完成数据交换。
1. Shuffle 文件的生成，是以 Map Task 为粒度的，Map 阶段有多少个 Map Task，就会生成多少份 Shuffle 中间文件。
2. 在生成中间文件的过程中，Spark 会借助一种类似于 Map 的数据结构，来计算、缓存并排序数据分区中的数据记录。这种 Map 结构的 Key 是（Reduce Task Partition ID，Record Key），而 Value 是原数据记录中的数据值。当 Map 结构被灌满之后，Spark 根据主键对 Map 中的数据记录做排序，然后把所有内容溢出到磁盘中的临时文件，如此往复，直到数据分区中所有的数据记录都被处理完毕。到此为止，磁盘上存有若干个溢出的临时文件，而内存的 Map 结构中留有部分数据，Spark 使用归并排序算法对所有临时文件和 Map 结构剩余数据做合并，分别生成 data 文件、和与之对应的 index 文件（用来标记目标分区所属数据记录的起始索引）。Shuffle 阶段生成中间文件的过程，又叫 Shuffle Write。
3. 不同的 Reduce Task 正是根据 index 文件中的起始索引来确定哪些数据内容是“属于自己的”。

### rdd 和block

Spark 存储系统负责维护所有暂存在内存与磁盘中的数据，这些数据包括 Shuffle 中间文件、RDD Cache 以及广播变量。[Spark存储系统原理与源码解析](https://mp.weixin.qq.com/s/1OUAvXgKuMlyAxcqcJc_3Q)

![](/public/upload/compute/spark_block.png)

[Spark自己的分布式存储系统BlockManager全解析](https://mp.weixin.qq.com/s/0XUeE6JWMRBSXSPJQFCmcw)BlockManager 是一个嵌入在 spark 中的 key-value型分布式存储系统，BlockManager 在一个 spark 应用中作为一个本地缓存运行在所有的节点上，BlockManager 对本地和远程提供一致的 get 和set 数据块接口，BlockManager 本身使用不同的存储方式来存储这些数据， 包括 memory, disk, off-heap。BlockManager内部会创建出MemoryStore和DiskStore对象用以存取block，如果内存中拥有足够的内存， 就 使用 MemoryStore存储，  如果 不够， 就 spill 到 磁盘中， 通过 DiskStore进行存储。

```scala
def putBlockData(blockId: BlockId,data: ManagedBuffer,level: StorageLevel,classTag: ClassTag[_]): Boolean = {...}
def get[T: ClassTag](blockId: BlockId): Option[BlockResult] = {...}
```

1. 在RDD层面上，RDD是由不同的partition组成的，所进行的transformation和action是在partition上面进行的；
2. 在storage模块内部，RDD又被视为由不同的block组成，对于RDD的存取是以block为单位进行的

本质上partition和block是等价的，只是看待的角度不同。在Spark storage模块中中存取数据的最小单位是block，所有的操作都是以block为单位进行的。RDD 的运算是基于 partition， 每个 task 代表一个 分区上一个 stage 内的运算闭包， task 被分别调度到 多个 executor上去运行， 那么是在哪里变成了 Block 呢，  我们以 spark 2.11 源码为准， 看看这个转变过程，一个 RDD 调度到 executor 上会运行调用 getOrCompute方法。
```scala
SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => {
      readCachedBlock = false
      computeOrReadCheckpoint(partition, context)
    })
```
如果当前RDD的storage level不是NONE的话，表示该RDD在BlockManager中有存储，那么调用CacheManager中的getOrCompute()函数计算RDD，在这个函数中partition和block发生了关系：首先根据RDD id和partition index构造出block id (rdd_xx_xx)，接着从BlockManager中取出相应的block。

