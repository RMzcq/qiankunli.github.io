---

layout: post
title: golang io
category: 技术
tags: Go
keywords: Go io

---

## 前言

* TOC
{:toc}

如果想兼顾开发效率，又能保证高并发，协程就是最好的选择。它可以在保持异步化运行机制的同时，用同步方式写代码（goroutine-per-connection），这在实现高并发的同时，缩短了开发周期，是高性能服务未来的发展方向。

对go netpoller 的信心：Getty 只考虑使用 Go 语言原生的网络接口，如果遇到网络性能瓶颈也只会在自身层面寻找优化突破点。

## 整体理念

[Go语言TCP Socket编程](https://tonybai.com/2015/11/17/tcp-programming-in-golang/)从tcp socket诞生后，网络编程架构模型也几经演化，大致是：“每进程一个连接” –> “每线程一个连接” –> “Non-Block + I/O多路复用(linux epoll/windows iocp/freebsd darwin kqueue/solaris Event Port)”。伴随着模型的演化，服务程序愈加强大，可以支持更多的连接，获得更好的处理性能。不过I/O多路复用也给使用者带来了不小的复杂度，以至于后续出现了许多高性能的I/O多路复用框架， 比如libevent、libev、libuv等，以帮助开发者简化开发复杂性，降低心智负担。不过Go的设计者似乎认为I/O多路复用的这种**通过回调机制割裂控制流的方式依旧复杂，且有悖于“一般逻辑”设计**，为此Go语言将该“复杂性”隐藏在Runtime中了：Go开发者无需关注socket是否是 non-block的，也无需亲自注册文件描述符的回调，只需在每个连接对应的goroutine中以“block I/O”的方式对待socket处理即可。

[The Go netpoller](https://morsmachine.dk/netpoller)In Go, **all I/O is blocking**. The Go ecosystem is built around the idea that you write against a blocking interface and then handle concurrency through goroutines and channels rather than callbacks and futures.An example is the HTTP server in the "net/http" package. Whenever it accepts a connection, it will create a new goroutine to handle all the requests that will happen on that connection. This construct means that the request handler can be written in a very straightforward manner. First do this, then do that. Unfortunately, using the blocking I/O provided by the operating system isn't suitable for constructing our own blocking I/O interface.

netty 在屏蔽java nio底层细节方面做得不错， 但因为java/jvm的限制，“回调机制割裂控制流”的问题依然无法避免。


## 原理

[Go 语言网络轮询器的实现原理](https://mp.weixin.qq.com/s/umYy7FCp1HAxgL83--4lRQ)

1. 多路复用 有赖于 linux  的epoll 机制，具体的说 是 epoll_create/epoll_ctl/epoll_wait 三个函数
2. epoll 机制包含 两个fd： epfd 和 待读写数据的fd（比如socket）。先创建efpd，然后向epfd 注册fd事件， 之后触发epoll_wait 轮询注册在epfd 的fd 事件发生了没有。 
2. netpoller 负责将 操作系统 提供的nio 转换为 goroutine 支持的blocking io。为屏蔽linux、windows 等底层nio 接口的差异，netpoller 定义一个 虚拟接口来封装底层接口。
    ```go
    func netpollinit()
    func netpollopen(fd uintptr, pd *pollDesc) int32
    func netpoll(delta int64) gList
    func netpollBreak()
    func netpollIsPollDescriptor(fd uintptr) bool
    ```

本文 主要讲 netpoller 基于 linux 的epoll 接口 的实现 [Go netpoller 网络模型之源码全面解析](https://zhuanlan.zhihu.com/p/299041493)

![](/public/upload/go/go_io.png)

Goroutine 让出线程并等待读写事件：当我们在文件描述符上执行读写操作时，如果文件描述符不可读或者不可写，当前 Goroutine 就会执行 `runtime.poll_runtime_pollWait` 检查 `runtime.pollDesc` 的状态并调用 `runtime.netpollblock` 等待文件描述符的可读或者可写。`runtime.netpollblock`会使用运行时提供的 `runtime.gopark` 让出当前线程，将 Goroutine 转换到休眠状态并等待运行时的唤醒。

**I/O 多路复用需要使用特定的系统调用/select**，java 语言需要显式调用select ，而golang 则通过netpoller 组件将select调用 重新隐藏了。

多路复用等待读写事件的发生并返回：**netpoller并不是由runtime中的某一个线程独立运行的**，runtime中的调度和系统调用会通过 runtime.netpoll 与网络轮询器交换消息，获取待执行的 Goroutine 列表，恢复Goroutine 为运行状态，并将待执行的 Goroutine 加入运行队列等待处理。

## io 前后的GPM
G1 正在 M 上执行，还有 3 个 Goroutine 在 LRQ 上等待执行。网络轮询器空闲着，什么都没干。

![](/public/upload/go/go_io_1.png)

G1 想要进行网络系统调用，因此它被移动到网络轮询器并且处理异步网络系统调用。然后，M 可以从LRQ 执行另外的 Goroutine。此时，G2 就被上下文切换到 M 上了。

![](/public/upload/go/go_io_2.jpg)

异步网络系统调用由网络轮询器完成，G1 被移回到 P 的 LRQ 中。一旦 G1 可以在 M 上进行上下文切换，它负责的 Go 相关代码就可以再次执行。

![](/public/upload/go/go_io_3.jpg)

执行网络系统调用不需要额外的 M。网络轮询器使用系统线程，它时刻处理一个有效的事件循环/eventloop。

## 实现

### 核心数据结构

**connect/accept/read/write 都会 转换为 pollDesc 操作**。

![](/public/upload/go/go_io_object.png)

调用 `internal/poll.pollDesc.init` 初始化文件描述符时不止会初始化网络轮询器，会通过 `runtime.poll_runtime_pollOpen` 函数重置轮询信息 `runtime.pollDesc` 并调用 `runtime.netpollopen` 初始化轮询事件。`runtime.netpollopen` 会调用 epollctl 向全局的轮询文件描述符 epfd 中加入新的轮询事件监听文件描述符的可读和可写状态


### 轮询 以获取 可执行的Goroutine 

这里类似 netty 的eventloop

```go
// src/runtime/netpoll_epoll.go
func netpoll(delay int64) gList {
    // 根据传入的 delay 计算 epoll 系统调用需要等待的时间；
	var waitms int32
	if delay < 0 {
		waitms = -1
	} else if delay == 0 {
		waitms = 0
	} else if delay < 1e6 {
		waitms = 1
	} else if delay < 1e15 {
		waitms = int32(delay / 1e6)
	} else {
		waitms = 1e9
    }
    var events [128]epollevent
retry:
    // 调用 epollwait 等待可读或者可写事件的发生；
	n := epollwait(epfd, &events[0], int32(len(events)), waitms)
	if n < 0 {
		if waitms > 0 {
			return gList{}
		}
		goto retry
    }
    // 在循环中依次处理 epollevent 事件；
    var toRun gList
	for i := int32(0); i < n; i++ {
		ev := &events[i]
		if *(**uintptr)(unsafe.Pointer(&ev.data)) == &netpollBreakRd {
			...
			continue
		}
		var mode int32
		if ev.events&(_EPOLLIN|_EPOLLRDHUP|_EPOLLHUP|_EPOLLERR) != 0 {
			mode += 'r'
		}
		...
		if mode != 0 {
			pd := *(**pollDesc)(unsafe.Pointer(&ev.data))
			pd.everr = false
			netpollready(&toRun, pd, mode)
		}
	}
	return toRun
```

计算了需要等待的时间之后，runtime.netpoll 会执行 epollwait 等待文件描述符转换成可读或者可写。当 epollwait 函数返回的值大于 0 时，就意味着被监控的文件描述符出现了待处理的事件。处理的事件总共包含两种，一种是调用 `runtime.netpollBreak` 函数触发的事件，该函数的作用是中断网络轮询器；另一种是其他文件描述符的正常读写事件，对于这些事件，我们会交给 `runtime.netpollready` 处理

### 代码实现

```go
//$GOROOT/src/net/tcpsock.go
type TCPConn struct {
    conn
}
//$GOROOT/src/net/net.go
type conn struct {
    fd *netFD
}
// $GOROOT/src/net/fd_unix.go
// Network file descriptor.
type netFD struct {
    pfd poll.FD 
    
    // immutable until Close
    family      int
    sotype      int
    isConnected bool // handshake completed or use of association with peer
    net         string
    laddr       Addr
    raddr       Addr
}  

// $GOROOT/src/internal/poll/fd_unix.go
// FD is a file descriptor. The net and os packages use this type as a
// field of a larger type representing a network connection or OS file.
type FD struct {
    // Lock sysfd and serialize access to Read and Write methods.
    fdmu fdMutex
    // System file descriptor. Immutable until Close.
    Sysfd int
    // I/O poller.
    pd pollDesc 
    // Writev cache.
    iovecs *[]syscall.Iovec
    ... ...    
}
```
net.conn只是*netFD 的外层包裹结构，最终 Write 和 Read 都会落在其中的fd字段上，netFD 在不同平台上有着不同的实现。

```go

// $GOROOT/src/internal/poll/fd_unix.go

func (fd *FD) Read(p []byte) (int, error) {
    if err := fd.readLock(); err != nil {
        return 0, err
    }
    defer fd.readUnlock()
    if len(p) == 0 {
        // If the caller wanted a zero byte read, return immediately
        // without trying (but after acquiring the readLock).
        // Otherwise syscall.Read returns 0, nil which looks like
        // io.EOF.
        // TODO(bradfitz): make it wait for readability? (Issue 15735)
        return 0, nil
    }
    if err := fd.pd.prepareRead(fd.isFile); err != nil {
        return 0, err
    }
    if fd.IsStream && len(p) > maxRW {
        p = p[:maxRW]
    }
    for {
        n, err := ignoringEINTRIO(syscall.Read, fd.Sysfd, p)
        if err != nil {
            n = 0
            if err == syscall.EAGAIN && fd.pd.pollable() {
                if err = fd.pd.waitRead(fd.isFile); err == nil {
                    continue
                }
            }
        }
        err = fd.eofError(n, err)
        return n, err
    }
}

func (fd *FD) Write(p []byte) (int, error) {
    if err := fd.writeLock(); err != nil {
        return 0, err
    }
    defer fd.writeUnlock()
    if err := fd.pd.prepareWrite(fd.isFile); err != nil {
        return 0, err
    }
    var nn int
    for {
        max := len(p)
        if fd.IsStream && max-nn > maxRW {
            max = nn + maxRW
        }
        n, err := ignoringEINTRIO(syscall.Write, fd.Sysfd, p[nn:max])
        if n > 0 {
            nn += n
        }
        if nn == len(p) {
            return nn, err
        }
        if err == syscall.EAGAIN && fd.pd.pollable() {
            if err = fd.pd.waitWrite(fd.isFile); err == nil {
                continue
            }
        }
        if err != nil {
            return nn, err
        }
        if n == 0 {
            return nn, io.ErrUnexpectedEOF
        }
    }
}
```

### 服务端一般处理逻辑

tcp 代码示例
```go
func handleConn(c net.Conn) {
    defer c.Close()
    for {
        // read from the connection
        // ... ...
        // write to the connection
        //... ...
    }
}
func main() {
    l, err := net.Listen("tcp", ":8888")
    if err != nil {
        fmt.Println("listen error:", err)
        return
    }
    for {
        c, err := l.Accept()
        if err != nil {
            fmt.Println("accept error:", err)
            break
        }
        // start a new goroutine to handle
        // the new connection.
        go handleConn(c)
    }
}
```

TCP 连接上的数据是一个没有边界的**字节流**，但在业务层眼中，没有字节流，只有各种协议消息。因此，无论是从客户端到服务端，还是从服务端到客户端，业务层在连接上看到的都应该是一个挨着一个的协议**消息流**。

对应到 代码上就是对 handleConn 进一步抽象/逻辑拆分，将业务逻辑转化到 handlePacket 上。handleConn 和 handlePacket 可以进一步拆分为 “粘包”（Frame） 和 “序列化”（Packet）两个部分
```
// handleConn的调用结构
read frame from conn
    ->frame decode
      -> handle packet
        -> packet decode
        -> packet(ack) encode
    ->frame(ack) encode
write ack frame to conn
```	
这个层次就很清晰了，**复杂逻辑/需求逐层分解**，也为分析类似 代码提供了分析的切入点（结构化思维）不会被带到细节里面去。 

```go
type Packet interface {
    Decode(io.Reader)(Packet,error)    
    Encode(io.Writer,Packet) error 
}
func handleConn(c net.Conn) {
    defer c.Close()
	packet = new ...
    for {
        // read from the connection
        packet, err := Decode(c)
		ackPacket, err := handlePacket(packet)
        // write to the connection
        err := Encode(c,ackPacket)
    }
}
```

## 常见优化

### 带缓存的网络 I/O

```
// var c net.Conn
rbuf := bufio.NewReader(c) 
wbuf := bufio.NewWriter(c)
```

bufio.Reader.Read 方法内部，每次从 net.Conn 尝试读取其内部缓存大小的数据，而不是用户传入的希望读取的数据大小。这些数据缓存在内存中，这样，后续的 Read 就可以直接从内存中得到数据，而不是每次都要从 net.Conn 读取，从而降低 Syscall 调用的频率。

### 重用内存对象

go tool pprof 可以观测占用内存最多的函数 和 代码（哪一行）。比如 每次服务端收到一个客户端 submit 请求时，都会在堆上分配一块内存表示 Submit 类型的实例

```go
s := Submit{}
// 改为
var SubmitPool = sync.Pool{
    New: func() interface{} {
        return &Submit{}
    },
}
s := SubmitPool.Get().(*Submit) // 从SubmitPool池中获取一个Submit内存对象
...
SubmitPool.Put(submit)          // 将submit对象归还给Pool池
```
## 上层封装——以getty 为例

[Go 语言网络库 getty 的那些事](https://mp.weixin.qq.com/s/z22k-E2ybjAMNtxzj5Aikw) 

go 通过netpoller 可以在保持异步化运行机制的同时，用同步方式写代码（goroutine-per-connection）。但毕竟代码拿到的还 是字节流，对于上层业务处理 编码工作量仍然非常大，于是 就像netty 之于java 一样 国内大佬 开发了 getty。Getty 严格遵循着分层设计的原则，主要分为数据交互层、业务控制层、网络层，同时还提供非常易于扩展的监控接口。 无论是基于getty 开发上层服务 还是 学习getty 之后再去 学习类似mosn/dubbo-go 等 都会大大降低 心智负担。

[Go 语言网络库 getty 的那些事](https://mp.weixin.qq.com/s/hihY7sEatJCHTTZHs3urfQ) 后续性能优化部分未细读

### 分层

![](/public/upload/go/getty_overview.png)

|分层|接口|用户需要做的工作|
|---|---|---|
|EventListener/业务处理|onOpen/onError/onClose/onMessage/onCron| 在onMessage 中写入业务逻辑|
|数据交互层/编解码|ReadWriter | 机型byte[] 和 Message 的转换|
|业务控制层|Connection/Session|
|网络层|Socket|

**见过的最好的协议层定义**：ReadWriter 接口定义代码如上。Read 接口之所以有三个返回值，是为了处理 TCP 流粘包情况：
- 如果发生了网络流错误，如协议格式错误，返回 (nil, 0, error)
- 如果读到的流很短，其头部 (header) 都无法解析出来，则返回 (nil, 0, nil)
- 如果读到的流很短，可以解析出其头部 (header) 但无法解析出整个包 (package)，则返回 (nil, pkgLen, nil)
- 如果能够解析出一个完整的包 (package)，则返回 (pkg, 0, error) 

session 负责客户端的一次连接建立的管理，会持有buffer、channel 等用于 工作流中 数据暂存、协程间沟通. session 的listen、accept方法返回值 都赋值给session 自己的成员，“自产自销”。
- 向下 Session 对 Go 内置的网络库做了完善的封装，包括对 net.Conn 的数据流读写、超时机制等。
- 向上，Session 提供了业务可切入的接口，用户只需实现 EventListener 就可以将 Getty 接入到自己的业务逻辑中。

Connection 根据不同的通信模式对 Go 内置网络库进行了抽象封装，Connection 分别有三种实现gettyTCPConn/gettyUDPConn/gettyWSConn。 PS Session Interface 和 session struct 都聚合了Connection，屏蔽不同的传输层差异， 可能是Connection 和 Session 拆开的原因。 

### 启动流程
在 Getty 中，server 服务的启动流程
```go
server := getty.NewTCPServer(options...)
server.RunEventLoop(newSession NewSessionCallback)
// type NewSessionCallback func(Session) error
// 用户需要通过该函数，为 session 设置好要用的 Reader、Writer 以及 EventListener。
func newSession(session getty.Session) error {
    ...
    session.SetPkgHandler(echoPkgHandler)       // 在 echoPkgHandler 中实现编解码
	session.SetEventListener(echoMsgHandler)    // 在onMessage 中实现业务逻辑
    ession.SetReadTimeout(conf.GettySessionParam.tcpReadTimeout)
	session.SetWriteTimeout(conf.GettySessionParam.tcpWriteTimeout)
    ...
}
```        
![](/public/upload/go/getty_server.png)

### 数据读取

RunEventLoop.RunEventLoop 收到新的连接 session 即执行 session.run
1. 处理byte[]: session.handlePackage ==> session.handleTCPPackage  
2. 处理协议消息: reader.Read 得到pkg ==>  session.addTask ==> session.listener.OnMessage 

```go
// github.com/AlexStocks/getty/transport/server.go
func (s *server) RunEventLoop(newSession NewSessionCallback) {
    err := s.listen()	  			// ==> server.listenTCP ==> server.streamListener = net.Listen("tcp", server.addr) 开启监听
	switch s.endPointType {
	case TCP_SERVER:
		s.runTcpEventLoop(newSession)
    ...
	}
}
func (s *server) runTcpEventLoop(newSession NewSessionCallback) {
	s.wg.Add(1)
	go func() {
		defer s.wg.Done()
		for {
			client, err = s.accept(newSession) // conn = server.streamListener.Accept(); session := newTCPSession(conn, server)
			client.(*session).run()
		}
	}()
}
// github.com/AlexStocks/getty/transport/session.go
func (s *session) run() {
    err := s.listener.OnOpen(s)
	go s.handleLoop()           // 发送网络字节流、调用 EventListener.OnCron() 执行定时逻辑
	go s.handlePackage()        // 读取字节数据，转为message，移交给   Goroutine Pool 处理业务逻辑
}
```
读取数据
```go
func (s *session) handlePackage() {
    ...
	err = s.handleTCPPackage()
    ...
}
func (s *session) handleTCPPackage() error {
	conn = s.Connection.(*gettyTCPConn)
	for {
	    bufLen, err = conn.recv(buf)
		pktBuf.Write(buf[:bufLen])
		for {
			pkg, pkgLen, err = s.reader.Read(s, pktBuf.Bytes())	// 将字节流转为pkg
            ...
			s.addTask(pkg)									    // 交给业务逻辑处理
			pktBuf.Next(pkgLen)
		}
	}
	return perrors.WithStack(err)
}
func (s *session) addTask(pkg interface{}) {
	f := func() {
		s.listener.OnMessage(s, pkg)
		s.incReadPkgNum()
	}
	if taskPool := s.EndPoint().GetTaskPool(); taskPool != nil { // 交给协程池执行
		taskPool.AddTask(f)
		return
	}
	f()	// 直接执行
}
```
### 发送数据
发送网络字节流、调用 `EventListener.OnCron()` 执行定时逻辑
```go
func (s *session) handleLoop() {
	for {
		select {
		case <-s.done: ...
		case outPkg, ok = <-s.wQ:
			iovec = iovec[:0]
			for idx := 0; idx < maxIovecNum; idx++ {
				pkgBytes, err = s.writer.Write(s, outPkg)   // 编码
				iovec = append(iovec, pkgBytes)
			}
			err = s.WriteBytesArray(iovec[:]...)
		case <-wheel.After(s.period):
			if flag {
				s.listener.OnCron(s)
			}
		}
	}
}
```

## 各个场景下的代码示例

### http 代码示例

[一文说透 Go 语言 HTTP 标准库](https://mp.weixin.qq.com/s/3JhRuTUxRc6gIcLIqHJ5Tw)

```go
func helloHandler(w http.ResponseWriter, req *http.Request) {
    io.WriteString(w, "hello, world!\n")
}
func main() {
    http.HandleFunc("/", helloHandler)
    http.ListenAndServe(":12345", nil)
}
```

### grpc 代码示例

demo
    helloworld
        helloworld.proto
        helloworld.pb.go ## 基于helloworld.proto 生成
    server
        main.go
    client
        main.go


服务端main.go 示例

    package main
    const (
        port = ":50051"
    )
    // server is used to implement helloworld.GreeterServer.
    type server struct{}
    // SayHello implements helloworld.GreeterServer
    func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {
        log.Printf("Received: %v", in.Name)
        return &pb.HelloReply{Message: "Hello " + in.Name}, nil
    }
    func main() {
        lis, err := net.Listen("tcp", port)
        if err != nil {
            log.Fatalf("failed to listen: %v", err)
        }
        s := grpc.NewServer()
        helloworld.RegisterGreeterServer(s, &server{})
        if err := s.Serve(lis); err != nil {
            log.Fatalf("failed to serve: %v", err)
        }
    }

helloworld.pb.go 中定义了RegisterGreeterServer 方法，除传入grpc.Server外，第二个参数是定义好的 GreeterServer interface。 由此可见，grpc 与java thrift 异曲同工

1. 定义thrift 文件
2. thrift 命令基于thrift 文件生成 对应语言的 代码文件，包含了服务 接口
3. 开发者提供 接口实现类





