---

layout: post
title: k8s批处理调度
category: 架构
tags: Kubernetes
keywords: kube-batch,volcano
---

## 简介

* TOC
{:toc}

[kube-batch](https://github.com/kubernetes-sigs/kube-batch)

如果一个podGroup minMember=10，且10个pod 正在运行，如果一个pod 失败，gang-scheduler 会有反应嘛？未完成。

## 必要性

[kube-batch在AI计算平台的应用](https://mp.weixin.qq.com/s/zXiSC0RWmow8RJ7XLog8JQ)k8s原生的调度器，会将需要启动的容器，放到一个优先队列（Priority Queue）里面，每次从队列里面取出一个容器，将其调度到一个节点上。 分布式训练需要所有worker都启动后，训练才能够开始进行。使用原生调度器，可能会出现以下问题：        
1. 一个任务包含了10个worker, 但是集群的资源只满足9个worker。原生调度器会将任务的9个worker调度并启动，而最后一个worker一直无法启动。这样训练一直无法开始，9个已经启动的worker的资源被浪费了。
2. 两个任务，各包含10个worker, 集群的资源只能启动10个worker。两个任务分别有5个worker被启动了，但两个任务都无法开始训练。10个worker的资源被浪费了。

能够将一个训练任务的多个worker当做一个整体进行调度，只有当任务所有worker的资源都满足，才会将容器在节点上启动；kube-batch还提供了队列的机制，同个队列的任务，会依次运行。不同队列之间可以设置优先级，优先级高的队列中的任务会优先得到调度。队列还可以设置权重，权重高的队列分配到的资源会更多。

## 案例
```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: qj-1
spec:
  backoffLimit: 6
  completions: 6
  parallelism: 6
  template:
    metadata:
      annotations:
        scheduling.k8s.io/group-name: qj-1
    spec:
      containers:
      - image: busybox
        imagePullPolicy: IfNotPresent
        name: busybox
        resources:
          requests:
            cpu: "1"
      restartPolicy: Never
	  ## 使用 kube-batch调度器
      schedulerName: kube-batch 
---
apiVersion: scheduling.incubator.k8s.io/v1alpha1
kind: PodGroup
metadata:
  name: qj-1
spec:
  minMember: 6
```
kube-batch 本身是一个是scheduler，从apiserver 获取pod信息，如果pod 的 schedulerName 不是kube-batch 就会ignore。

虽然我们使用kube-batch是为了gang-scheduler（至少笔者是这样），kube-batch 作为一个调度器，基本的“为pod 选择一个最合适的node/node间pod 数量尽可能均衡/抢占” 这些特性还是要支持的。因此在设计上，即便不需要 像default scheduler 那么灵活，至少在代码层面要方便扩展，方便塞入个性化的调度需求。

Action 实现了调度机制（mechanism），Plugin 实现了调度的不同策略（policy）。
1. Reclaim:  负责将任务中满足回收条件的容器删除。
2. Allocate: 负责将还未调度的设置了资源限制（request、Limit）的容器调度到节点上。
3. Backfill:  负责将还未调度的的没设置资源限制的容器调度到节点上。
4. Preempt: 负责将任务中满足条件的容器抢占。

### 源码分析

尝试以数据结构 + 算法的方式理解下，未完成。

```go
func (pc *Scheduler) Run(stopCh <-chan struct{}) {
	// Start cache for policy.
	go pc.cache.Run(stopCh)
	pc.cache.WaitForCacheSync(stopCh)
	// Load configuration of scheduler
	pc.actions, pc.plugins, err = loadSchedulerConf(schedConf)
	go wait.Until(pc.runOnce, pc.schedulePeriod, stopCh)
}
func (pc *Scheduler) runOnce() {
	ssn := framework.OpenSession(pc.cache, pc.plugins)
	defer framework.CloseSession(ssn)
	for _, action := range pc.actions {
		actionStartTime := time.Now()
		action.Execute(ssn)
	}
}
```

plugin 会根据自己的语义 注册相关的函数到 Session中，在Action.Execute 中被调用。

```go
type Session struct {
	UID types.UID
	cache cache.Cache
	Jobs    map[api.JobID]*api.JobInfo
	Nodes   map[string]*api.NodeInfo
	Queues  map[api.QueueID]*api.QueueInfo
	Backlog []*api.JobInfo
	Tiers   []conf.Tier
	plugins          map[string]Plugin
	eventHandlers    []*EventHandler
	jobOrderFns      map[string]api.CompareFn	// 对job 排序
	queueOrderFns    map[string]api.CompareFn	// 对queue 排序
	taskOrderFns     map[string]api.CompareFn	// task 排序
	predicateFns     map[string]api.PredicateFn		// 预选 
	preemptableFns   map[string]api.EvictableFn		// 优选
	reclaimableFns   map[string]api.EvictableFn		// 判断task 是否可以被回收
	overusedFns      map[string]api.ValidateFn		
	jobReadyFns      map[string]api.ValidateFn		// job 是否ready
	jobPipelinedFns  map[string]api.ValidateFn		
	jobValidFns      map[string]api.ValidateExFn
	nodePrioritizers map[string][]priorities.PriorityConfig
}
// kube-batch/pkg/scheduler/framework/session_plugins.go
func (ssn *Session) AddJobReadyFn(name string, vf api.ValidateFn) {...}
func (ssn *Session) JobReady(obj interface{}) bool {...jobValidFns...}
func (ssn *Session) AddJobValidFn(name string, fn api.ValidateExFn) {...}
func (ssn *Session) JobValid(obj interface{}) *api.ValidateResult {...jobReadyFns...}
// kube-batch/pkg/scheduler/framework/session.go
func (ssn *Session) Allocate(task *api.TaskInfo, hostname string) error {...}
func (ssn *Session) Pipeline(task *api.TaskInfo, hostname string) error {...}
func (ssn *Session) Evict(reclaimee *api.TaskInfo, reason string) error {...}
```
**Session 一共有两类方法**：session_plugins，与plugin 相关的各种Function 注入与调用；真正操作Pod的Allocate/Pipeline/Evict。Action.Execute中，Action 依次遍历 pending 状态的task，根据session_plugins方法判断task 和job 状态，最终调用Pod的Allocate/Pipeline/Evict。这或许是Action 和Plugin ，机制和策略分离的一种解释。

[kube-batch 具体如何实现gang scheduler](https://www.jianshu.com/p/0c191c4aeb5a)
[kube-batch 从代码中找出gang scheduler这个过程](https://www.jianshu.com/p/9c19e4bb061a)

### gang-scheduler

gang-scheduler 非常类似分布式事务/tcc，tcc 有一个预留的动作，要实现gang-scheduler的效果，Pod 自带的Pending/Running/Succeeded/Failed/Unknown 是不够的， 为此Pod 对应struct TaskInfo 定义了Pending/Allocated/Pipelined/Binding/Bound/Running/Releasing/Succeeded/Failed/Unknown 状态，其中 Allocated 用来标记pod 已分配资源但未实际运行的状态。

一个podGroup 对应一个JobInfo，kube-batck 将pod 转换为taskInfo，每一个node对应NodeInfo，所谓 为pod分配Node：taskInfo.NodeName=nodeName，NodeInfo减去pod 标定的资源。当发现 JobInfo 下的taskInfo 符合minMember，即真正为 pod 赋值nodeName。具体代码还要再捋捋。

```go
func (alloc *allocateAction) Execute(ssn *framework.Session) {
	...
	// 对queue和job 进行排序  queues 和jobs 都是优先级队列
	for {
		if queues.Empty() {break}
		queue := queues.Pop().(*api.QueueInfo)	// 取出优先级最高的queue
		if ssn.Overused(queue) {continue} // 某个queue 占用的资源过多，不再为其pod进行调度了
		jobs, found := jobsMap[queue.UID]	 // 取出queue 对应的jobs
		job := jobs.Pop().(*api.JobInfo)
		// 赋值api.Pending状态的task 到  pendingTasks
		tasks := pendingTasks[job.UID]
		glog.V(3).Infof("Try to allocate resource to %d tasks of Job <%v/%v>",
		tasks.Len(), job.Namespace, job.Name)
		for !tasks.Empty() {
			task := tasks.Pop().(*api.TaskInfo)
			predicateNodes := util.PredicateNodes(task, allNodes, predicateFn)  // 预选
			if len(predicateNodes) == 0 {break} 
			priorityList, err := util.PrioritizeNodes(task, predicateNodes, ssn.NodePrioritizers()) // 优选
			if err != nil {break} 
			nodeName := util.SelectBestNode(priorityList)
			node := ssn.Nodes[nodeName]
			// Allocate idle resource to the task.
			if task.InitResreq.LessEqual(node.Idle) {
				if err := ssn.Allocate(task, node.Name); err != nil {...} // 绑定task到node
			} else {  
				//store information about missing resources
				job.NodesFitDelta[node.Name] = node.Idle.Clone()
				job.NodesFitDelta[node.Name].FitDelta(task.InitResreq)
				// Allocate releasing resource to the task if any.
				if task.InitResreq.LessEqual(node.Releasing) {
				if err := ssn.Pipeline(task, node.Name); err != nil {...}
				}
			}
			// job ready（比如job一共10个 minMember=5）当前job 放在jobs的最后，还剩的5的pod 调度优先级就不高了，可以放放，暂停对这个job的调度
			if ssn.JobReady(job) && !tasks.Empty() {  
				jobs.Push(job)
				break
			}
		}
		queues.Push(queue)  // Added Queue back until no job in Queue.
	}
}
```
`Session.Allocate` ==>  `if ssn.JobReady(job)  Session.dispatch`==>  `Cache.Bind(task *api.TaskInfo, hostname string) error` 真正 更新pod 即设定pod.nodeName。 
1. 对于jobReadyFns 来说，只有gang plugin 注册了jobReadyFns 到Session 上，Session.JobReady 默认返回true。也就是，如果不用gang plugin，则每一次 Session.Allocate，job ready默认为true，为pod 真正分配node。
2. 用了gang plugin之后，则每次Session.Allocate，要先校验 gang.jobReadyFns，校验通过，则为pod 真正分配node，否则只是将 task 标记为 api.Allocated ，记住了taskInfo 所属的nodeInfo，并在nodeInfo 中扣掉了taskInfo的资源`NodeInfo.addTask`。
3. 比如job一共10个 minMember=5，如果已经伪分配了4个，第5找不到合适的节点。gang plugin 向Session 注册了 ReclaimableFn/PreemptableFn。对于 这种状态的 job，这4个task 可以被抢占后回收。

## Volcano（未完成）

[Volcano 在 Kubernetes 中运行高性能作业实践](https://time.geekbang.org/dailylesson/detail/100033217)


[一文带你解读Volcano架构设计与原理](https://segmentfault.com/a/1190000039122753)

![](/public/upload/kubernetes/volcano_overview.png)

## 案例

![](/public/upload/kubernetes/volcano_example_yml.jpeg)

kube-batch 是pod 自己建，Volcano Controller 依据JobSpec创建依赖的Pod， Service， ConfigMap等资源，执行配置的插件，并负责Job后续的生命周期管理(状态监控，事件响应，资源清理等)。Volcano Scheduler监听Pod资源的创建，依据策略，完成Pod资源的调度和绑定。

[使用Kubeflow和Volcano实现典型AI训练任务](https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0075.html)通过简单的设置schedulerName字段的值为“volcano”，启用Volcano调度器

```yml
kind: TFJob
metadata:
  name: {train_name}  
spec:
  schedulerName: volcano
  tfReplicaSpecs:
    Ps:
      replicas: {num_ps}
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          serviceAccount: default-editor
          containers:
          - name: tensorflow
            command:
            ...
            env:
            ...
            image: {image}
            workingDir: /opt
          restartPolicy: OnFailure
    Worker:
      replicas: 1
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          serviceAccount: default-editor
          containers:
          - name: tensorflow
            command:
            ...
            env:
            ...
            image: {image}
            workingDir: /opt
          restartPolicy: OnFailure
```

[安装kubeflow tfjob并让 搭配 volcano 的教程](https://www.jianshu.com/p/99c622cc1284)