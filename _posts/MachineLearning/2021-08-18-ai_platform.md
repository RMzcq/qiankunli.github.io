---

layout: post
title: AI云平台
category: 架构
tags: MachineLearning
keywords:  ai platform

---

## 简介

* TOC
{:toc}

《人工智能云平台原理、设计与应用》近年来涌现了很多智能算法，这些算法需要软件的支撑，没有软件的支撑，理论很难与应用相结合，新硬件也很难为应用提速，所以巨头们推出了TensorFlow、Pytorch等开源框架，然而这些框架不足以支撑人工智能全流程生产化应用，它们仅面向个人开发者和研究人员，管理少数计算设备资源，无法在云计算资源上提供面向多租户的智能应用全流程服务。欠缺诸如海量样本数据管理与共享存储、集群管理、任务调度、快速训练与部署、运行时监控等能力，缺乏人工智能生产流程的抽象、定义和规范，导致用户形成生产力的成本过高。

海量数据标注 + 大规模计算 + 工程化（python或c++）=AI系统，也被称为MLaaS/MLOps。

![](/public/upload/machine/ai_develop_progress.png)

AI 平台可能是云原生技术栈上最具"可玩性"的一种场景。种类繁多的异构资源和通讯形式、调度策略、常规业务不需要的拓扑感知、花式任务编排、数据密集型有状态应用、天然离/在线混部。

## 《MLOps实践》

我们开发ML模型依赖于几个要素，如数据、算法或参数，在实验过程中，这些要素会随着时间的推移而改变，从而生成不同的版本。创建数据和参数版本镜像可以帮助我们跟踪不同的版本，但版本控制有其自身的成本。世面上有很多书讲算法原理、如何训练ML模型，也有很多书讲如何构建软件项目，但很少有书把这两个世界融合起来，对于如何构建由ML驱动实际应用的项目工程方面，如数据收集、存储、模型部署、管理以及监控运维等方面的书却很少见。

ML 是一个通过算法和统计模型从数据中学习知识的学科，当我们遇到的问题可以用一套可管理的确定性规则（且随着数据变化并不需要变更规则）来解决时，这类问题便不需要ML。

DevOps依靠工具、自动化和工作流程来抽象软件工程的复杂性，让开发人员专注于需要解决的实际问题，在软件开发领域已经基本成为标配，那为什么这套方法论或经验不能直接应用到ML领域呢？其原因在于，ML的**跨领域特性**延伸出了新的维度，比如**增加了一个额外的数据维度**
1. 对于传统软件，几乎可以即时体现代码变化对结果的影响，但在ML中，想到看到代码变化对结果的影响需要重新训练模型
2. 对于传统软件，一个版本的代码产生一个版本的软件，在版本控制系统的辅助下，我们可以在任何时候创建应用程序的任意变体。在ML中，**开发的结果不是代码而是模型**，而这个模型又是由创建和训练模型的代码版本及其所使用的数据产生。代码和数据分别处在两个平行的平面上，它们之间共享时间维度，但在所有其它方面都是独立的。
3. 在ML 中，不仅要保存不同版本的代码，还需要一个地方来保存不同版本的数据和模型工件，和涉及的元数据信息。与代码不同，模型性能会随着时间的推移而衰退，这就需要监控。一旦发现预测准确率下降， 就需要新的数据重新训练模型，**训练永远不会结束**，即ML的迭代属性。
ML的迭代属性意味着， 如果没有完备的自动更新流程，想要通过手动的方式来反映这些变化需要大量的工作，这是一个系统工程。

|实践|DevOps|DataOps|MLOps|
|---|---|---|---|
|版本控制|代码版本化|数据版本化|代码版本化<br/>数据版本化<br/>模型版本化|
|管道|n/a|数据处理管道<br>ETL|训练管道<br>服务管道|
|行为验证|单元测试|单元测试|模型验证和测试|
|数据验证|n/a|数据格式及业务逻辑验证|统计验证|
|CI/CD|将代码部署至生产环境|将数据处理管道部署至生产环境|部署代码及训练管道至生产环境|
|监控|SLO|SLO|SLO<br>异常监控<br>统计监控|

## 业界实践

### 虎牙

[互动直播场景下的AI基础设施建设](https://time.geekbang.org/qconplus/detail/100059720)

手动时代

![](/public/upload/machine/huya_ai_manual.png)

AI平台的定位：面向算法工程师，围绕AI 模型的全生命周期去提供一个一站式的机器学习服务平台。

![](/public/upload/machine/huya_platform_1.png)
![](/public/upload/machine/huya_platform_2.png)


### 腾讯

[高性能深度学习平台建设与解决业务问题实践](https://time.geekbang.org/qconplus/detail/100059719)构建公司统一的大规模算力，方便好用的提供GPU

![](/public/upload/machine/tecent_platform_2.png)

### 阿里

[KubeDL 加入 CNCF Sandbox，加速 AI 产业云原生化](https://mp.weixin.qq.com/s/7SUhnW4cnk_3G9Q7lIytcA)

从算法工程师着手设计第一层神经网络结构，到最终上线服务于真实的应用场景，除 AI 算法的研发外还需要大量基础架构层面的系统支持，包括数据收集和清理、分布式训练引擎、资源调度与编排、模型管理，推理服务调优，可观测等。众多系统组件的协同组成了完整的机器学习流水线。

具体的说
1. 数据抽取组件->实现样本数据筛选
2. 画像组件->实现样本数据与画像字段的关联
3. 特征组件->实现画像数据到特征数据格式的转换
4. 切分组件->实现样本抽样
5. 深度组件->实现用户自定义模型训练
6. 预测组件->验证模型指标
7. 部署组件->模型部署上线
8. 串联上述所有组件的**工作流组件**，以支持算法工程师 模型训练部署的整个过程。PS: 具体实现上设计到 自己实现或使用工作流引擎。
自动化能自动的，加速能加速的，减少模型耗时，加快算法工程师的产出效率，并以此为前提提高资源利用率。 

[摆脱 AI 生产“小作坊”：如何基于 Kubernetes 构建云原生 AI 平台](https://mp.weixin.qq.com/s/yGc44Q0qseDG7zy0-PC8gg)在初期，用户利用 Kubernetes，Kubeflow，nvidia-docker 可以快速搭建 GPU 集群，以标准接口访问存储服务，自动实现 AI 作业调度和 GPU 资源分配，训练好的模型可以部署在集群中，这样基本实现了 AI 开发和生产流程。紧接着，用户对生产效率有了更高要求，也遇到更多问题。比如 GPU 利用率低，分布式训练扩展性差，作业无法弹性伸缩，训练数据访问慢，缺少数据集、模型和任务管理，无法方便获取实时日志、监控、可视化，模型发布缺乏质量和性能验证，上线后缺少服务化运维和治理手段，Kubernetes 和容器使用门槛高，用户体验不符合数据科学家的使用习惯，团队协作和共享困难，经常出现资源争抢，甚至数据安全问题等等。从根本上解决这些问题，AI 生产环境必然要从“单打独斗的小作坊”模式，向“资源池化+AI 工程平台化+多角色协作”模式升级。我们将云原生 AI 领域聚焦在两个核心场景：持续优化异构资源效率，和高效运行 AI 等异构工作负载。
1. 优化异构资源效率
2. 运行 AI 等异构工作负载，兼容 Tensorflow，Pytorch，Horovod，ONNX，Spark，Flink 等主流或者用户自有的各种计算引擎和运行时，统一运行各类异构工作负载流程，统一管理作业生命周期，统一调度任务工作流，保证任务规模和性能。一方面不断提升运行任务的性价比，另一方面持续改善开发运维体验和工程效率。

![](/public/upload/machine/ai_platform_tech.png)

用户体验：对于数据科学家和算法工程师开发训练 AI 模型来说，Kubernetes 的语法和操作却是一种“负担”。他们更习惯在 Jupyter Notebook 等 IDE 中调试代码，使用命令行或者 Web 界面提交、管理训练任务。任务运行时的日志、监控、存储接入、GPU 资源分配和集群维护，最好都是内置的能力，使用工具就可以简单操作。因此要提供命令行工具/SDK/运维大盘/开发控制台来满足用户的各种需要。 [AI 作业生命周期管理（Arena）](https://mp.weixin.qq.com/s/yGc44Q0qseDG7zy0-PC8gg)

### vivo

训练平台 + 推理平台 + 容器平台（日常维护cli ==> 白屏化）

[一站式机器学习平台在 vivo AI 的实践](https://www.infoq.cn/article/thlkstomylrgxl2hzm8w)

算力的易用性
1. 分布式训练
2. 交互式调试
3. 容量托管
4. 训练sdk

算力的灵活调度
1. 基于容器的资源调度
2. 在离线统一资源池
3. 混合云 [vivo AI 计算平台的 ACK 混合云实践](https://mp.weixin.qq.com/s/O7y6kr01Au-T8M0kah7D9w)
4. 基于GPU 拓扑的调度
 
算力的高效利用
1. 资源分配：弹性伸缩、算力超卖（多个容器使用一张卡，GPU隔离）
2. 资源使用：训练/推理加速，训练容错
3. 弹性训练 [vivo AI 计算平台弹性分布式训练的探索和实践](https://www.infoq.cn/article/EhRjlkwxs6C6cT4cHzlt)
4. 训练性能剖析
5. GPU 远程调用
6. 数据编排和加速

[vivo推荐中台升级路：机器成本节约75%，迭代周期低至分钟级](https://mp.weixin.qq.com/s/fpmepb75j_Qr0UlXR6YnQg)玲珑·推荐中台主要为数据及算法工程师提供从算法策略到 A/B 实验的工程架构解决方案、通用的特征服务和样本生产服务、模型的离线训练到上线部署全生命周期管理、高性能推理等能力。玲珑·推荐中台包含四大模块：推荐中心、特征中心、模型中心、端云协同推荐。

![](/public/upload/machine/vivo_ai_platform.png)

具体一点

![](/public/upload/machine/vivo_ai_platform_overview.png)


### 腾讯

[腾讯般若系统](https://mp.weixin.qq.com/s/NISDTSjrHCRSHbjkH5b-Bg)

[cube-studio](https://github.com/tencentmusic/cube-studio)腾讯音乐直接开源的一个ai训练平台。

### 美团

[美团外卖特征平台的建设与实践](https://mp.weixin.qq.com/s/CWY7RQcfidkvAAQCI5kRKg)

![](/public/upload/machine/meituan_ai.png)

## 基础设施

1. 工作流编排
2. AI 任务可视化，实际的 AI 训练过程包含数据处理、模型训练、模型评估、模型部署和发布等一系列过程，每个 AI 算法工作者针对每个步骤都会用自己的方式去临时管理，这样会导致代码和模型管理混乱，难以追踪和复现结果，也无法进行高效分享和代码复用。

### 工作流

[美团外卖特征平台的建设与实践](https://mp.weixin.qq.com/s/CWY7RQcfidkvAAQCI5kRKg)平台化建设最重要的流程之一是“如何进行流程抽象”，业界有一些机器学习平台的做法是平台提供较细粒度的组件，让用户自行选择组件、配置依赖关系，最终生成一张样本构建的DAG图。对于用户而言，这样看似是提高了流程编排的自由度，但深入了解算法同学实际工作场景后发现，算法模型迭代过程中，大部分的样本生产流程都比较固定，反而让用户每次都去找组件、配组件属性、指定关系依赖这样的操作，会给算法同学带来额外的负担，所以我们尝试了一种新的思路来优化这个问题：模板化 + 配置化，即平台提供一个基准的模板流程，该流程中的每一个节点都抽象为一个或一类组件，用户基于该模板，通过简单配置即可生成自己样本构建流程。PS：就是多提供了一个模板？

[大规模运行 Apache Airflow 的经验和教训](https://mp.weixin.qq.com/s/KEAHAiqV4iqdO9hUW8fu6w)Apache Airflow 是一个能够开发、调度和监控工作流的编排平台。未细读

[深入探索云原生流水线的架构设计](https://mp.weixin.qq.com/s/P57XYUnxE2gxA8VIBzX-TQ)
1. 在 Pipeline 中，我们对一个任务执行的抽象是 ActionExecutor。一个执行器只要实现单个任务的创建、启动、更新、状态查询、删除等基础方法，就可以注册成为一个 ActionExecutor。
    1. Engine 层负责流水线的推进，包括：Queue Manager 队列管理器，支持队列内工作流的优先级动态调整、资源检查、依赖检查等。Dispatcher 任务分发器，用于将满足出队条件的流水线分发给合适的 Worker 进行推进。Reconciler 协调器，负责将一条完整的流水线解析为 DAG 结构后进行推进，直至终态。
    2. 模块内部使用插件机制，对接各种任务运行时。
2. 在一条流水线中，节点间除了有依赖顺序之外，一定会有数据传递的需求。上下文传递，后置任务可以引用前置任务的“值”和“文件”
3. Pipeline 之所以好用，是因为它提供了灵活一致的流程编排能力，并且可以很方便地对接其他**单任务执行平台**，这个平台本身不需要有流程编排的能力。调度时，根据任务类型智能调度到对应的任务执行器上，包括 K8sJob、Metronome Job、Flink Job、Spark Job 等等。
4. 这里简单列举一些比较常见的功能特性：
    1. 配置即代码
    2. 扩展市场丰富
    3. 可视化编辑
    4. 支持嵌套流水线
    5. 灵活的执行策略，支持 OnPush / OnMerge 等触发策略
    6. 支持工作流优先队列
    7. 多维度的重试机制
    8. 定时流水线及定时补偿功能
    9. 动态配置，支持“值”和“文件”两种类型，均支持加密存储，确保数据安全性
    10. 上下文传递，后置任务可以引用前置任务的“值”和“文件”
    11. 开放的 OpenAPI 接口，方便第三方系统快速接入

[超大模型工程化实践打磨，百度智能云发布云原生 AI 2.0 方案](https://mp.weixin.qq.com/s/uwkSgIhEDP2XwZnpnVVpLg)针对 DAG 运行全方面优化，提升 DAG 执行效率
1. 通过节点产出数据统一管理，**自动跳过未发生变化的节点**，缩短 DAG 运行时间。
2. 通过感知节点产出临时数据位置，减少中间数据读写的频率和远程访问频率。
3. 通过 AI 资源调度的异构能力，检测节点的运行环境（CPU、GPU 等）按需调度不同节点，降低算力占用成本。

### 模板市场

[腾讯音乐cube-studio开源一站式云原生机器学习平台](https://mp.weixin.qq.com/s/6uaUFS01W2lxnM-SU4PsfQ)直接使用airflow/argo等调度组件，但没有编排界面，直接编辑yaml也很麻烦。所以我们单独开发了**模板市场**和pipeline编排工具，并在开源中提供多种分布式模板。用户通过拖拉拽方式编排pipeline，配置执行参数（模板需要设置参数）后就可运行。模板市场的模板是注册进去的，用户和平台都可操作。流程比较简单：准备镜像，标注清楚该镜像的参数、类型、限制条件、用户提示等，使用标准化的注册流程注册至平台后，平台用户就可使用该模板。模板开发者多为平台方或使用方组织架构内特定工程人员。PS：**pipeline 和模板是分不开的**。ai工作流的模板一般分为四类：数据导入；分布式训练；模型校验；模型部署。

### 分布式加速（待补充）

[腾讯音乐cube-studio开源一站式云原生机器学习平台](https://mp.weixin.qq.com/s/6uaUFS01W2lxnM-SU4PsfQ)pipeline搭建好后会发现虽然编排流程简化了，但是运行时间并没有减少，耗时主要集中在数据处理和训练上。数据处理分为结构化和非结构化数据处理，对于结构化数据处理，像sql等形式目前还是采用公司已有的大数据spark平台。对于非结构化数据处理/训练以及结构化数据的训练部分的耗时，考虑使用**分布式加速**的方式。

平台对分布式训练的优化，分为四块
1. 在用户代码层面，深入进程内部进行优化；
2. 在数据层面，避免数据倾斜、优化数据加载；例如一个任务提前分布好每个worker的内容，当数据分配不均匀和部分worker受其他机器worker影响时，性能会下降，此时每个worker剩余任务数量就不一样，导致用户任务迟迟不能完成，不能推进其他事情。
3. 物理层面，优化大文件、带宽问题；
4. 在此之上，使用共享gpu提升使用率，动态cpu算力调整，上层优化任务调度，亲密性，多项目组的资源共享等。

### 存储加通信

[腾讯音乐cube-studio开源一站式云原生机器学习平台](https://mp.weixin.qq.com/s/6uaUFS01W2lxnM-SU4PsfQ)平台侧封装的模板可以定向优化，部分通用性模板开放性较高，不对用户逻辑做限制，仅对用户提供分布式能力，此类模板用户的训练性能可能存在io瓶颈，很难将io优化全部下放至用户代码层进行优化，所以我们在io层做了一层全局优化。从cfs切换到ssd ceph，最终性能达到G级别的写入和7-8G级别的读取，满足大部分训练需求。

随后发现cpu使用率明显上升，但此时网络索引问题成为下一个需要的解决的问题，特别是音视频领域。在推荐场景下，一般是csv格式的大文件（10G+），数量相对少，但是在音视频文本领域，存在上千万的大量小文件，容易卡在高频率的网络请求上。平台的文件存储在远端的分布式存储中，但是计算集群可能是不同网络的私有集群。在当前网段新建ssd ceph，抵消网络异地或者跨网段时延。定向优化后，训练性能在GPU上再提升3倍以上。

经过存储优化后，可以看到io的耗时占比在链路中明显下降，但是通信时延占比超过总耗时的55%。PS：如何分析一个任务的io耗时、通信耗时？

### 资源利用率

1. 对于cpu分布式任务，用户可自己借助多进程、协程提升单个pod的cpu利用率。此部分倾向于让用户申请更多worker数提升性能。对于用户没有主动优化cpu利用的情况，支持通过系统的监控和智能调整优化方案将该任务的资源申请值调整至合理的范围，进而提升cpu使用率。
2. gpu比较特殊，平台在训练过程中对gpu的占用为整卡占用的方式，因为在训练中使用vgpu非常容易出现卡零碎浪费的情况，并且即使使用vgpu，并处理好零碎卡的问题，提升了平台整体gpu利用率，但任务耗时没有降低。故平台方倾向于提升用户占用的gpu卡的单卡利用率，进而提升单个分布式任务的运行效率。
3. gpu利用率低的核心问题是gpu等待时间太长，可能cpu处理或io等操作，包括优化磁盘存储、数据加载、网络通信、预处理、cpu上的模型保存。

    ![](/public/upload/machine/ai_workflow.png)

对用户完全自行开发的代码，平台方会根据监控配合用户进行针对性优化，提升gpu的利用率。例如对临时性的高频文件使用内存映射磁盘（libariry库）；训练框架io加载的并行参数优化；计算和数据同域分布；音视频的小文件合并成大文件；专用并行io库；cpu数据处理和gpu计算分隔成两个任务处理，降低cpu和gpu切换开销；使用gpu来进行处理；batchsize调整把gpu打满。

对于一些场景，平台或用户不能投入人力定向优化，比如推荐中cpu & gpu混合任务场景，更倾向于使用共享gpu的方案。在云原生多机多卡训练中，大部分框架每个worker默认会占用卡的全部显存。但是因为代码处理可能并不能将单卡的核全部利用起来，这种场景可以配置单个卡上跑更多worker，对应的整个环境变量的配置跟随变动，例如pytorch的WORLD_SIZE，RANK，LOCAL_RANK。至于每张卡上启动多少个进程来共享同一张卡，需要结合最先达到瓶颈的资源，考虑cpu和gpu的配比情况来看，比如对推荐场景，一般是cpu先达到瓶颈。gpu机器设备上cpu资源相对少的特性（3个纯cpu机器算力有200多核，但4卡gpu机器只有60个核cpu），cpu和gpu是一个相对匹配的算力。在共享gpu的方案中，根据哪一个资源（cpu/gpu）优先达到瓶颈，就可结束进程增加。

### Scheduler调度

1. 首先批调度能力，引入kube-batch的gang
2. 另外是亲密度和调度算法的调整：对cpu型任务倾向于把不同的任务分配到不同的cpu机器上避免单机瓶颈；对gpu任务，多个任务尽量分配到同一个gpu机器上，减少网络通信消耗；对同一pipeline中的不同任务，尽量部署到不同的机器上，避免存在相似任务任务达到单机瓶颈。对于不同的pipeline，放到算力相对空闲的机器上，平衡集群使用率。

## 其它

深度学习平台的搭建，将遇到诸多挑战，主要体现在以下方面：

1. 数据管理自动化。在深度学习的业务场景中，从业人员会花费大量的时间获取和转换建立模型需要的数据。数据处理过程中还将产生新的数据，这些数据不单单应用于本次训练，很可能用于后续推理过程。并且新 生成的数据不需要传给数据源，而是希望放置在新的存储空间。这需要基础平台提供可扩展的存储系统。PS： 手动命令行、页面、代码上传、下载，pod 随时可以访问，分布式训练任务多pod 之间也要共享一些临时的存储文件。[ AI 数据编排与加速（Fluid）](https://mp.weixin.qq.com/s/yGc44Q0qseDG7zy0-PC8gg) **因为训练任务实例都是临时创建的**，实例刚创建时没有任务、数据文件，需要训练脚本自己下载，数据流转很不方便。同时多部门、部门内人员的任务、数据文件也需要分目录隔离和共享。
2. 资源的有效利用。深度学习相关的应用是资源密集型，资源使用波动大，存在峰值和谷值。在应用开始运行的时候，快速获取计算资源；在应用结束后，回收不适用的计算资源对于资源利用率的提升相当重要。数据处理、模型训练和推理所使用的计算资源类型和资源占用时间有所不同，这就需要计算平台提供弹性的资源供应机制。
3. 屏蔽底层技术的复杂性

如何跟公有云协同 [vivo AI 计算平台的 ACK 混合云实践](https://mp.weixin.qq.com/s/O7y6kr01Au-T8M0kah7D9w)

AI 中台具备六大能力。
1. 统一的存储空间，支持多数据源导入。
2. Pipeline 可视化工作流管理与执行，支持数据科学家从数据建模阶段开始的可视化管理，节省成本，快速体现数据科学家的价值。[深入探索云原生流水线的架构设计](https://mp.weixin.qq.com/s/P57XYUnxE2gxA8VIBzX-TQ)
3. 基于容器的计算资源分配和软件库安装，支持 TensorFlow、PyTorch 等各种框架。
4. 支持 GPU、TPU、CPU 框架和基于异构计算的模型管理。
5. 模型管理，支持新手快速上手，无需通过自己实现原始算法，只需要理解算法原理就可以通过调参实现。
6. AI Serving，模型一键封装为 API，一键部署。

[大规模机器学习与AutoML技术](https://mp.weixin.qq.com/s/jWnR_7s1F8EM2NushXMCLA)建模流程包括数据—特征—算法—调参—评估，经过这个流程得到模型，然后评估模型还差，如果不好，通过反馈重新进行这个过程。在这个过程中有很多数据、特征和算法环节(数据抽样、数据去噪、特征选择、特征变换等)，每个环节都可能影响模型效果，整个建模过程就是这些环节反复调整，直到得到模型。AutoML借助计算机搜索的优点，将整个过程在计算机上自动实现，难点就是将这个问题数学化，然后就有了目标函数和方向。

![](/public/upload/machine/ml_process.png)

AutoML问题定义，通俗地来说，假设有这么一个过程：我们有一个训练集、一组参数，然后训练出一个模型，利用测试集评测模型，根据评测结果来看参数效果好坏。我们希望整个过程能够自动化，这就是AutoML。AutoML技术挑战包括超参结构复杂、目标函数不可导、评估代价巨大。
1. 超参结构复杂
2. 目标函数不可导，在机器学习中如果可导，采用简单的随机梯度下降方法就能解决；
3. AutoML利用AI评估AI，每评估一次就需要把模型训练一次，代价非常大。