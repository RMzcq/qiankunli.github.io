---

layout: post
title: 机器学习泛谈
category: 技术
tags: MachineLearning
keywords: 深度学习

---


## 前言

吴恩达：假如有一件事是一个正常人可以在1s以下做到的，我们就可以使用人工智能自动做。 假如你可以拿到一个具体重复发生的事情的海量数据， 你就可以用这些数据来预测下一次的结果。 

[陆奇最新演讲：没有学习能力，看再多世界也没用](https://mp.weixin.qq.com/s?__biz=MzIxNTAzNzU0Ng==&mid=2654628648&idx=1&sn=e8d8f9d2389a2f6b4069fd3642360320&chksm=8c50a82dbb27213b04d7b28bf5e94980b477ce16d3cf0078fc315fd654af159b4903df8a779a&mpshare=1&scene=1&srcid=#rd) 绝对高屋建瓴，推荐多看几遍。

[火了这么久的 AI，现在怎么样了？](https://mp.weixin.qq.com/s/yJCDcScQRDlsfyonFr0qyA)机器学习的目标是利用有限的样本对未知的目标函数求近似。任何机器学习模型都有三个 component 组成，首先确定要学习的函数空间、然后确定使用的数据，用哪些训练数据拟合机器学习模型，最后是找到优化算法，让机器从函数空间中学习到最好的模型，即最佳匹配数据的模型。机器学习是考虑所有可能的函数，而深度学习只考虑一个特殊类的函数，神经网络。

## 规律一直都在，但如何找到规律：算法 vs 数据相关性

吴军《见识》：在达尔文研究进化论的同时，奥地利的教士孟德尔从另一个角度开始研究生命的奥秘和物种之间的关联，并且最终发现了遗传的规律性。在他之后，美国科学家摩尔根确认了细胞内的染色体承载着物种的遗传物质，奠定了现代遗传学。在二战后，英美科学家一起确定了遗传物质DNA的双螺旋分子结构，从此破解了生命的奥秘，也了解到基因突变对物种变异和进化的影响。这些现代生物学和遗传学的结果，都支持了生物具有共同祖先的说法。不仅生物的遗传物质都是DNA，而且构成生命所需的蛋白质也保持了一致性，例如核糖体、DNA聚合酶和RNA聚合酶，不但出现在较原始的细菌里，也出现在复杂的动物体内。这些蛋白质的核心部分在不同生物中具有相似的构造和功能。PS：生物学和遗传学上的“一生二、二生三、三成无穷”。万事万物，都有一个共同的本源


为什么要深度学习?从计算机编程的角度讲,解决问题的手段一般有两种

1. 将"规则"代码化
2. 穷举,利用"规则"干掉不符合条件的

问题是，对于有些东西，无法用规则来描述，或者即便能够描述，对计算能力的要求也过高。这迫使人们使用新的方法来解决问题，即学习人脑的思维方式.

《智能商业》如果我们将数据看做数据时代的一桶高标号汽油，那算法无疑就是这台引擎。机器学习是算法一次决定性的跃升，也正是在这次跃升中，数据对算法的巨大被充分显现出来。**任何一个算法模型，尤其是能够自我学习、自我优化的算法模型，都承担着在成千上万可能的因素中寻找出所隐藏的联系的艰巨任务**。工业革命使得体力劳动自动化，信息革命使得脑力劳动自动化，而机器学习使得自动化过程本身自动化。

## 人脑神经网络

[神经元之间的连接网络](http://www.xlzx.com/cgi/xr_html/articles/NLP/2796.html)

一个人在出生之前，脑中的1000亿个神经元已经几乎全部准备好，而神经元之间的连接网络则是十分稀疏的。因为婴儿未能意识思考，故此，他只会凭外界的刺激而制造连接网络。

任何声音、景物、身体活动，只要是新的(第一次)，都会使得脑里某些神经元的树突和轴突生长，与其他神经元连接，构成新的网络。同样的刺激第二次出现时，会使第一次建立的网络再次活跃。就是说，新网络只能在有新刺激的情况下产生。一个人的一生之中，不断有新的网络产生出来，同时有旧的网络萎缩、消失。

一个旧的网络，对同样的刺激会特别敏感，每次都会比前一次启动得更快、更有力。多次之后，这个网络便会深刻到成为习惯或本能了。这便是学习和记忆的成因。


## AI 在国内的发展


1. 中国最近兴起了一个产业，活跃于十八线县城及农村， 比如给你几十万张图片，标记出图片中所有的垃圾桶。然后位于北京的人工智能it 公司通过机器学习 就可以识别各种垃圾桶。这其实就是模拟了人的学习过程，一开始认为蓝色的是垃圾桶，后来发现跟颜色没关系。后来认为圆的也是垃圾桶， 后来发现跟形状没关系，等见得足够多，机器就会有一个模糊的认知：能装东西的、较深的、桶形都可以是垃圾桶。
2. 一个技术分享，分享人提到人工智能的当前阶段：有多少人工，就有多少智能。

[如何看待张潼老师离职腾讯？ - 姚冬的回答 - 知乎](https://www.zhihu.com/question/307359849/answer/566414432) 中国过去三十年，IT行业应用的技术基本都是美国那边已经成熟了的技术，已经在欧美普遍使用，甚至有些已经形成盈利性产业了。AI 是我们第一次和全球同步遇到一次新技术浪潮，AI技术在欧美也没有成熟，中国的IT企业其实基本没有落后多少，我们第一次感受到了新技术发展初期带来的各种问题。比如 新技术在实践中比旧技术表现还差，新技术不可靠，新技术成本太高，找不到落地用途等等。AI技术很新，也就意味着问题很多，但是并不意味着技术没前途，只是需要些时间去发展完善。

邵浩：大家现在都在谈人工智能技术，而且很多人都会把人工智能和 AlphaGo 以及深度学习划上等号。其实人工智能涵盖的学科范围是非常广泛的，包括心理学、神经科学、哲学、认知科学等等。我们目前看到的大量成果都只是深度学习和大数据的化学反应。而且，大量的人工智能应用还都是人工 + 智能，离真正的认知智能差距甚远。如何利用技术赋能产品，得到用户和资本的认可，才是最重要的

## 影响

### 对人的影响

王天一：无人超市和无人工厂的出现都表明：人工智能的真正威胁在于使绝大多数人沦为机器的附庸。人工智能本质上是一种劳动工具，但当劳动工具已经强大到反客为主时，作为劳动者的人类便成了多余的角色，有降格为“亚人工智能”的风险。如何应对？一个方法是专精于依赖创造力的领域，比如科学和艺术，但这对天赋要求较高，显然不适用于每一个人。另一种门槛更低的办法就是掌握核心技术，“知己知彼”，让人工智能回到“为我所用”的工具性。 

### 对架构的影响

[从技术演变的角度看互联网后台架构](https://mp.weixin.qq.com/s/7Qc8irbh0rz43OPWKbO2Ag)到了2017之后，前面千奇百怪的后端体系基本上都趋同了。Kafka的实时消息队列，Spark的流处理（当然现在也可以换成Flink，不过大部分应该还是Spark），然后后端的存储，基于Hive的数据分析查询，然后根据业务的模型训练平台。各个公司反正都差不多这一套，在具体细节上根据业务有所差异，或者有些实力强大的公司会把中间一些环节替换成自己的实现，不过不管怎么千变万化，整体思路基本都一致了。个人认为，Machine Learning的很大一个好处，是简化业务逻辑，简化后台流程，不然一套业务一套实现，各种数据和业务规则很难用一个整体的技术平台来完成。

![](/public/upload/algorithm/machine_learning_on_architecture.JPG)

在传统后端系统中，业务逻辑其实和数据是客观分离的，逻辑规则和数据之间并不存在客观联系，而是人为主观加入，并没形成闭环，如上图左上所示。而基于机器学习的平台，这个闭环就形成了，从业务数据->AI模型->业务逻辑->影响用户行为->新的业务数据这个流程是自给自足的。这在很多推荐系统中表现得很明显，通过用户行为数据训练模型，模型对页面信息流进行调整，从而影响用户行为，然后用新的用户行为数据再次调整模型。而在机器学习之前，这些观察工作是交给运营人员去手工猜测调整。PS：从图示看，不是完全接管系统（与用户直接交互），而是接管系统的配置部分

现代的后端数据处理越来越偏向于DAG的形态，Spark不说了，DAG是最大特色；神经网络本身也可以看作是一个DAG（RNN其实也可以看作无数个单向DNN的组合）；TensorFlow也是强调其Graph是DAG，另外编程模式上，Reactive编程也很受追捧。无论如何，数据，数据的跟踪Tracking，数据的流向，是现代后台系统的核心问题，只有Dataflow和Data Pipeline清晰了，整个后台架构才会清楚。


## why deep

[周志华：“深”为什么重要，以及还有什么深的网络](https://mp.weixin.qq.com/s/U6DvnuLogfmfYzwe5ULmiQ)

1. 深度学习就等于深度神经网络吗？深度学习是机器学习中使用深度神经网络的的子领域。所以如果我们要谈深度学习的话，是绕不开深度神经网络的。
2. 神经网络需要可微的函数、需要能够计算梯度，这是最根本最重要的
3. 我们知道一个机器学习模型，它的复杂度实际上和它的容量有关，而容量又跟它的学习能力有关。所以就是说学习能力和复杂度是有关的。机器学习界早就知道，如果我们能够增强一个学习模型的复杂度，那么它的学习能力能够提升。那怎么样去提高复杂度，对神经网络这样的模型来说，有两条很明显的途径。一条是我们把模型变深，一条是把它变宽。如果从提升复杂度的角度，那么变深是会更有效的。当你变宽的时候，你只不过是增加了一些计算单元，增加了函数的个数，在变深的时候不仅增加了个数，其实还增加了它的嵌入的程度。既然你们早就知道要建立更深的模型了？那为什么现在才开始做？这就涉及到另外一个问题，我们把机器学习的学习能力变强了，这其实未必是一件好事。因为机器学习一直在斗争的一个问题，就是经常会碰到过拟合（overfit）。我希望学到的是一般规律，能够用来预测未来的事情。但是有时候我可能把这个样本数据本身的一些独特特性学出来了，而不是一般规律。错误地把它当成一般规律来用的时候，会犯巨大的错误。那现在我们为什么可以用很复杂的模型？其实我们设计了许多方法来对付过拟合，比如神经网络有 dropout、early-stop 等。但有一个因素非常简单、非常有效，那就是用很大的数据。
4. 为什么深度神经网络能成功？就是因为复杂度大。如果从复杂度这个角度去解释的话，我们就没法说清楚为什么扁平的（flat），或者宽的网络做不到深度神经网络的性能？实际上我们把网络变宽，虽然它的效率不是那么高，但是它同样也能起到增加复杂度的能力。但是这样的模型在应用里面怎么试，我们都发现它不如深度神经网络好。所以我们要问这么一个问题：深度神经网络里面最本质的东西到底是什么？今天我们的回答是，本质是表征学习的能力。以往我们用机器学习解决一个问题的时候，首先我们拿到一个数据，比如说这个数据对象是个图像，然后我们就用很多特征把它描述出来，比如说颜色、纹理等等。这些特征都是我们人类专家通过手工来设计的，表达出来之后我们再去进行学习。而今天我们有了深度学习之后，现在不再需要手工去设计特征了。你把数据从一端扔进去，结果从另外一端就出来了，中间所有的特征完全可以通过学习自己来解决。所以这就是我们所谓的特征学习，或者说表征学习。我们都认可这和以往的机器学习技术相比可以说是一个很大的进步，这一点非常重要。我们不再需要依赖人类专家去设计特征了。这个过程中的关键点是什么呢？是逐层计算，layer-by-layer processing。
5. 当我们有很大的训练数据的时候，这就要求我们必须要有很复杂的模型。否则假设我们用一个线性模型的话，给你 2000 万样本还是 2 亿的样本，其实对它没有太大区别。它已经学不进去了。而我们有了充分的复杂度，恰恰它又给我们使用深度模型加了一分。所以正是因为这几个原因，我们才觉得这是深度模型里面最关键的事情。
6. 这是我们现在的一个认识：第一，我们要有逐层的处理；第二，我们要有特征的内部变换；第三，我们要有足够的模型复杂度。这三件事情是我们认为深度神经网络为什么能够成功的比较关键的原因。或者说，这是我们给出的一个猜测。那如果满足这几个条件，我们其实马上就可以想到，那我不一定要用神经网络。凡是用过深度神经网络的人都会知道，你要花大量的精力来调它的参数，其实在图像上面调参数的经验，在语音问题上基本上不太有借鉴作用。所以当我们跨任务的时候，这些经验可能就很难共享。还有很多问题，比如说我们在用深度神经网络的时候，模型复杂度必须是事先指定的。因为我们在训练这个模型之前，我们这个神经网络是什么样就必须定了，然后我们才能用 BP 算法等等去训练它。其实这会带来很大的问题，因为我们在没有解决这个任务之前，我们怎么知道这个复杂度应该有多大呢？所以实际上大家做的通常都是设更大的复杂度。比如说 Kaggle 上面的很多竞赛有各种各样的真实问题，有买机票的，有订旅馆的，有做各种的商品推荐等等，我们就可以看到在很多任务上的胜利者并不是神经网络，它往往是像随机森林，像 xgboost 等等这样的模型。深度神经网络获胜的任务，往往就是在图像、视频、声音这几类典型任务上，都是连续的数值建模问题。而在别的凡是涉及到混合建模、离散建模、符号建模这样的任务上，其实深度神经网络的性能可能比其他模型还要差一些。这也就是我们说的「没有免费的午餐定理」，已经有数学证明，一个模型不可能在所有任务中都得到最好的表现。
7. 我们从学术的观点来总结一下，今天我们谈到的深度模型基本上都是深度神经网络。如果用术语来说的话，它是多层、可参数化的、可微分的非线性模块所组成的模型，而这个模型可以用 BP 算法来训练。那么这里面有两个问题。第一，我们现实世界遇到的各种各样的问题的性质，并不是绝对都是可微的，或者用可微的模型能够做最佳建模的。第二，过去几十年里面，我们的机器学习界做了很多模型出来，这些都可以作为我们构建一个系统的基石，而中间有相当一部分模块是不可微的。





