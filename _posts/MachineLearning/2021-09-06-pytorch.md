---

layout: post
title: pytorch
category: 架构
tags: MachineLearning
keywords:  pytorch

---

## 简介

* TOC
{:toc}


[这是我见过最好的NumPy图解教程！](https://mp.weixin.qq.com/s/77aq0JQs8SX6molSxewOdQ)NumPy是Python中用于数据分析、机器学习、科学计算的重要软件包。它极大地简化了向量和矩阵的操作及处理。《用python实现深度学习框架》用python 和numpy 实现了一个深度学习框架MatrixSlow
1. 支持计算图的搭建、前向、反向传播
2. 支持多种不同类型的节点，包括矩阵乘法、加法、数乘、卷积、池化等，还有若干激活函数和损失函数。比如向量操作pytorch 跟numpy 函数命名都几乎一样
3. 提供了一些辅助类和工具函数，例如构造全连接层、卷积层和池化层的函数、各种优化器类、单机和分布式训练器类等，为搭建和训练模型提供便利

安装 [Ubuntu 20.04 下的 PyTorch（CPU版）安装及配置](https://www.limfx.pro/readarticle/1662/ubuntu-2004-xia-de-pytorchcpuban-an-zhuang-ji-pei-zhi-)

## 什么是pytorch

[Pytorch详细介绍(上)](https://mp.weixin.qq.com/s/YDNX49NGWMqkLML0_k0Phg)Pytorch是一个很容易学习、使用和部署的深度学习库。PyTorch同时提供了更高性能的C++ API（libtorch）。Pytorch使用Tensor作为核心数据结构，Tensor（张量）是神经网络世界的Numpy，其类似与Numpy数组，但是又不同于Numpy数组。为了提高Tensor的计算速度，有大量的硬件和软件为了Tensor运算进行支持和优化。总得来说，Tensor具有如下特点：

1. 具有类似Numpy的数据，Tensor可以与Numpy共享内存；
2. 可以指定计算设备，例如，设定Tensor是在CPU上计算，还是在GPU上计算；
3. Tensor可微分。
4. 通过访问张量的device属性可以获取张量所在的设备

PyTorch工作流以及与每个步骤相关联的重要模块

![](/public/upload/machine/pytorch_process.png)


## 数据读取

PyTorch 使用 Dataset 类与 DataLoader 类的组合，来得到数据迭代器。
1. Dataset用来表示数据集。通过继承 Dataset 类来自定义数据集的格式、大小和其它属性，后面就可以供 DataLoader 类直接使用。
2. 如果数据量很大，考虑到内存有限、I/O 速度等问题，在训练过程中不可能一次性的将所有数据全部加载到内存中，也不能只用一个进程去加载，所以就需要多进程、迭代加载，而 DataLoader 就是基于这些需要被设计出来的。DataLoader 是一个迭代器，最基本的使用方法就是传入一个 Dataset 对象，它会根据参数 batch_size 的值生成一个 batch 的数据，节省内存的同时，它还可以实现多进程、数据打乱等处理。

```python
from torch.utils.data import DataLoader
tensor_dataloader = DataLoader(dataset=my_dataset, # 传入的数据集, 必须参数
                               batch_size=2,       # 输出的batch大小
                               shuffle=True,       # 数据是否打乱
                               num_workers=0)      # 进程数, 0表示只有主进程

# 以循环形式输出
for data, target in tensor_dataloader: 
    print(data, target)
```


Torchvision 库中的torchvision.datasets包中提供了丰富的图像数据集的接口。常用的图像数据集，例如 MNIST、COCO 等，这个模块都为我们做了相应的封装。
它的工作方式是先从网络上把数据集下载到用户指定目录，然后再用它的加载器把数据集加载到内存中。最后，把这个加载后的数据集作为对象返回给用户。

```python

# 以MNIST为例
import torchvision
mnist_dataset = torchvision.datasets.MNIST(root='./data',
                                       train=True,
                                       transform=None,
                                       target_transform=None,
                                       download=True)
```

## 模块

模块本身是一个类nn.Module，PyTorch的模型通过继承该类，在类的内部定义子模块的实例化，通过前向计算调用子模块，最后实现深度学习模型的搭建

```python
import torch.nn as nn
class LinearModel(nn.Module):
    def __init__(self,ndim):
        super(LinearModel,self).__init__()
        self.ndim = ndim
        # 需要使用nn.Parameter来包装这些参数，使之成为子模块（仅仅由参数构成的子模块），这是因为在后续训练的时候需要对参数进行优化，只有将张量转换为参数才能在后续的优化过程中被优化器访问到。
        self.weight = nn.Parameter(torch.randn(ndim,1)) # 定义权重
        self.bias = nn.Parameter(torch.randn(1)) # 定义偏置
    def forward(self,x):
        # y = Wx +b
        return x.mm(self.weight) + self.bias
```

## 部署

[如何部署 PyTorch 模型](https://zhuanlan.zhihu.com/p/344364948) TorchServe

## 其它

PyTorch Hub，通过PyTorch Hub，用户可以获得一系列预训练的深度学习模型，主要包括计算机视觉、自然语言处理、生成式模型和音频模型等，这些预训练模型的出现能有效地加快用户开发新型的深度学习模型，方便用户构建基线模型和复现深度学习模型的效果。

[PyTorch Lightning工具学习](https://mp.weixin.qq.com/s/LeUjxmGHpIrUsLDBmKNrdA)Pytorch Lightning是在Pytorch基础上进行封装的库(可以理解为keras之于tensorflow)，为了让用户能够脱离PyTorch一些繁琐的细节，专注于核心代码的构建，提供了许多实用工具，可以让实验更加高效。 文中以MNIST 为例列出了 使用原生pytorch 和PyTorch Lightning 的代码对比。 



